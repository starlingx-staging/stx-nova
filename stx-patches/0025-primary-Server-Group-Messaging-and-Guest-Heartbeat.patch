From 5140088fa4a67f4bc2a465b5d48ff553a77d1f70 Mon Sep 17 00:00:00 2001
From: Jack Ding <jack.ding@windriver.com>
Date: Thu, 24 Aug 2017 14:04:11 -0400
Subject: [PATCH 025/143] primary: Server Group Messaging and Guest Heartbeat

This commits ports the server group messaging extension.  This
extension allows applications running on instances that are part of a
server group to broadcast messages to, receive status notifications and
query status from other servers in the group.

- Messaging is enabled by extra spec sw:wrs:srv_grp_messaging and
  implemented using virtio-serial device.
- App to Agent layer messages are JSON encoded.
- Greenthreads are spawned for handle_server_grp_msg as well as
  broadcast_server_grp_msg and server_grp_status_query to avoid stalls
  of handle_server_grp_msg greenthread.
- send_server_grp_msg() is synchronized to make sure the unix socket can
  only be written to by one eventlet at a time.
- Server group notifications are added to the instance state update
  path so that state updates (e.g. pause.start, pause.end) are sent to
  other instances in the same server group.
- Host will send NACK msgs to guest when there are parsing or protocol
  errors.

This commit also enables a dedicated backchannel connection between
guest and host to be used for heartbeat.  It's implemented using the
virtio-serial device and is conditional based on whether heartbeating is
enabled via the flavor extra spec sw:wrs:guest:heartbeat=true

Validation of extra specs sw:wrs:srv_grp_messaging, sw:wrs:guest:heartbeat
and sw:wrs:auto_recovery for true/false value is also done.

This commit merges the following R3 commits:

c632dab Port Server Group Messaging Extensions to mitaka
f6889db Fix up Messaging unit tests
20643b8 port to mitaka for commit 4d7e24d
    Enable backend channel connection for heartbeat
5d15c53 Guest HB is not established sometimes even it is set to True
a0beb87 add validation check in flavor_extraspec

1111136 Creation of a 2nd VM in Server Group fails
   Class ComputeAPI method send_server_group_msg() still uses self.client
   to set client, and must now use self.router to do that.

8176c17 optimize server group notification overhead
   The VIM would timeout if 10 to 15 servers in a servergroup were being rebuilt.
   This was because the server group messaging was expensive.
   This fix adds two optimizations
   1) Do not send the rpc message unless the appropriate server group messaging
   extra spec is set
   2) Use cast instead of call.
   This reduces the time to send rebuild messages for 16 servers from 50 seconds
   down to about 10.

66129a7 Avoid unnecessary database accesses in send_server_group_msg
   The reason for the database contention in this Nokia test case,
   is that given the number of VMs in the server group (+80), the database
   is overflowed with send_server_group_msg requests. In this specific
   scenario where no VM is listening to this kind of messages (no VM has
   'sw:wrs:srv_grp_messaging' set in its flavor extra_spec), this is a
   waste of accesses to the database.
   This proposed fix adds a field to the server group's metadetails
   directory, named 'wrs-sg:active_listener', that is set whenever there is
   at least one VM listening to these messages. When that is not the case,
   send_server_group_msg() simply returns without pounding the database by
   requesting each VM in the group's  flavor.

f60d729 Pike rebase bug 265 InstanceNotFound when scheduling failed
   Server group messages should not be sent if the originating instance
   has not been created in the database or if the instance does not belong
   to a server group.

__TYPE_primary
__TAG_servergroup,backchannel,cgcsmessaging,heartbeat
__R4_commit_d3d521f
__R3_commit_89412e0
__TC2916,TC5133
---
 nova/api/openstack/compute/flavors_extraspecs.py   |  15 ++
 nova/compute/cgcs_messaging.py                     | 267 +++++++++++++++++++++
 nova/compute/manager.py                            |  11 +
 nova/compute/rpcapi.py                             |  14 +-
 nova/compute/utils.py                              |  17 ++
 nova/conductor/api.py                              |  20 ++
 nova/conductor/manager.py                          | 115 ++++++++-
 nova/conductor/rpcapi.py                           |  23 ++
 nova/notifications/base.py                         |  10 +
 nova/rpc.py                                        |   1 +
 .../unit/api/openstack/compute/test_services.py    |   4 +
 nova/tests/unit/compute/test_compute.py            |  15 +-
 nova/tests/unit/compute/test_compute_mgr.py        |   6 +-
 nova/tests/unit/conductor/test_conductor.py        |   5 +
 nova/tests/unit/notifications/test_base.py         |  31 ++-
 nova/tests/unit/test_baserpc.py                    |  11 +-
 nova/tests/unit/test_test.py                       |  12 +-
 nova/tests/unit/virt/test_virt_drivers.py          |  13 +
 nova/virt/libvirt/driver.py                        |  35 +++
 19 files changed, 614 insertions(+), 11 deletions(-)
 create mode 100644 nova/compute/cgcs_messaging.py

diff --git a/nova/api/openstack/compute/flavors_extraspecs.py b/nova/api/openstack/compute/flavors_extraspecs.py
index 05f24b1..2d92f86 100644
--- a/nova/api/openstack/compute/flavors_extraspecs.py
+++ b/nova/api/openstack/compute/flavors_extraspecs.py
@@ -236,6 +236,20 @@ class FlavorExtraSpecsController(wsgi.Controller):
                 msg = _('%(k)s is no longer supported.') % {'k': key}
                 raise webob.exc.HTTPBadRequest(explanation=msg)
 
+    # extra spec sw:keys validation
+    @staticmethod
+    def _validate_sw_keys(flavor):
+        keys = ['sw:wrs:auto_recovery', 'sw:wrs:srv_grp_messaging',
+                'sw:wrs:guest:heartbeat']
+        specs = flavor.extra_specs
+        for key in keys:
+            if key in specs:
+                value = specs[key].lower()
+                if value not in ('false', 'true'):
+                    msg = _('%(k)s must be True or False, value: %(v)s') \
+                             % {'k': key, 'v': value}
+                    raise webob.exc.HTTPBadRequest(explanation=msg)
+
     @staticmethod
     def _validate_nested_vmx(flavor):
         key = 'hw:wrs:nested_vmx'
@@ -267,6 +281,7 @@ class FlavorExtraSpecsController(wsgi.Controller):
         self._validate_cpu_policy(flavor)
         self._validate_cpu_thread_policy(flavor)
         self._validate_numa_node(flavor)
+        self._validate_sw_keys(flavor)
         self._validate_nested_vmx(flavor)
         self._validate_cpu_realtime_mask(flavor)
         self._validate_wrs_deprecated_keys(flavor)
diff --git a/nova/compute/cgcs_messaging.py b/nova/compute/cgcs_messaging.py
new file mode 100644
index 0000000..4f7be40
--- /dev/null
+++ b/nova/compute/cgcs_messaging.py
@@ -0,0 +1,267 @@
+#
+# cgcs_messaging.py, this forwards messages between nova-compute and guests
+#
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+# Copyright (c) 2013-2016 Wind River Systems, Inc.
+#
+#
+#
+#
+#
+
+import nova.context
+from oslo_log import log as logging
+from oslo_serialization import jsonutils
+from oslo_utils import timeutils
+import socket
+
+from nova import utils
+
+LOG = logging.getLogger(__name__)
+
+# unix socket address we bind to
+server_grp_name = 'cgcs.server_grp'
+
+
+# The server group message is JSON encoded null-terminated string without
+# embedded newlines.
+
+# message_keys
+GRP_DATA = "data"
+GRP_DEST_ADDR = "dest_addr"
+GRP_DEST_INSTANCE = "dest_instance"
+GRP_LOG_MSG = "log_msg"
+GRP_MSG_TYPE = "msg_type"
+GRP_ORIG_MSG_TYPE = "orig_msg_type"
+GRP_SEQ = "seq"
+GRP_SOURCE_ADDR = "source_addr"
+GRP_SOURCE_INSTANCE = "source_instance"
+GRP_VERSION = "version"
+
+# corresponds to server_group_msg_type
+GRP_BROADCAST = "broadcast"
+GRP_NACK = "nack"
+GRP_NOTIFICATION = "notification"
+GRP_STATUS_QUERY = "status_query"
+GRP_STATUS_RESP = "status_response"
+GRP_STATUS_RESP_DONE = "status_response_done"
+GRP_UNKNOWN = "unknown"
+
+# currently only support one version
+GRP_CURRENT_VERSION = 2
+
+# current max message size
+GRP_MAX_MSGLEN = 4096
+
+
+class CGCSMessaging(object):
+    def __init__(self, compute_task_api):
+        self._do_setup(compute_task_api)
+
+    # this as a helper makes it easier to mock out for testing
+    def _do_setup(self, compute_task_api):
+        self.server_grp_sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
+        server_grp_address = '\00' + server_grp_name
+        self.server_grp_sock.bind(server_grp_address)
+        nova.utils.spawn_n(self.handle_server_grp_msg, compute_task_api)
+
+    @staticmethod
+    def instance_name_to_id(instance_name):
+        """Get the instance ID.  instance_name should look like
+           "instance-xxxxxxxx" where xxxxxxxx is the instance id in the
+           database, and is in hex.
+        """
+        # strip off the "instance-" at the front
+        instance_id_str = instance_name.partition('-')[2]
+        # strip off anything after the trailing null
+        instance_id_str = instance_id_str.partition(b'\0')[0]
+        # convert to int assuming it's in hex
+        instance_id = int(instance_id_str, base=16)
+        return instance_id
+
+    def broadcast_server_grp_msg(self, context, compute_task_api,
+                                 s_instance, data):
+        """Send message coming from the cgcs messaging backchannel daemon
+           (originating from up in the guest) to all other servers in server
+           group.
+        """
+        outmsg = dict()
+        outmsg[GRP_VERSION] = GRP_VERSION
+        outmsg[GRP_MSG_TYPE] = GRP_BROADCAST
+        outmsg[GRP_SOURCE_INSTANCE] = s_instance
+        outmsg[GRP_DATA] = data
+
+        LOG.debug("sending broadcast msg %s", repr(outmsg))
+
+        # get the instance ID.
+        instance_id = self.instance_name_to_id(s_instance)
+
+        # Send to nova-conductor where we can do database lookups directly
+        compute_task_api.send_server_group_msg(context, outmsg,
+                                               instance_id=instance_id)
+
+    def server_grp_status_query(self, context, compute_task_api, s_instance,
+                                seqnum):
+        """The specified instance wants the current status of all the other
+           instances in the server group.
+        """
+        # get the instance ID.
+        instance_id = self.instance_name_to_id(s_instance)
+        # Send to nova-conductor where we can do database lookups directly
+        statuslist = compute_task_api.get_server_group_status(context,
+                                                              instance_id)
+        for status in statuslist:
+            outmsg = dict()
+            outmsg[GRP_VERSION] = GRP_CURRENT_VERSION
+            outmsg[GRP_MSG_TYPE] = GRP_STATUS_RESP
+            outmsg[GRP_SEQ] = seqnum
+            outmsg[GRP_DATA] = status
+            LOG.debug("sending group_status_query msg %s", repr(outmsg))
+
+            # send the message to the requesting instance
+            self.send_server_grp_msg(s_instance, outmsg)
+
+        # Send "status resp complete" message
+        outmsg = dict()
+        outmsg[GRP_VERSION] = GRP_CURRENT_VERSION
+        outmsg[GRP_MSG_TYPE] = GRP_STATUS_RESP_DONE
+        outmsg[GRP_SEQ] = seqnum
+        outmsg[GRP_DATA] = status
+        LOG.debug("sending group_status_query_done msg %s", repr(outmsg))
+
+        self.send_server_grp_msg(s_instance, outmsg)
+
+    def send_server_grp_nack(self, s_instance, msg_type, log_msg):
+        """Sends a nack to guest to indicate parsing or protocol failure when
+           handling messages from guest.
+        """
+        outmsg = dict()
+        outmsg[GRP_VERSION] = GRP_CURRENT_VERSION
+        outmsg[GRP_MSG_TYPE] = GRP_NACK
+        outmsg[GRP_ORIG_MSG_TYPE] = msg_type
+        outmsg[GRP_LOG_MSG] = log_msg
+        LOG.debug("sending nack msg %s", repr(outmsg))
+
+        # send the message to the requesting instance
+        self.send_server_grp_msg(s_instance, outmsg)
+
+    def handle_server_grp_msg(self, compute_task_api):
+        """Receive messages coming from the cgcs messaging backchannel daemon
+           (originating from up in the guest).
+        """
+        context = nova.context.get_admin_context()
+
+        while True:
+            inmsg = self.server_grp_sock.recv(GRP_MAX_MSGLEN)
+            if not inmsg:
+                continue
+            LOG.debug("got datagram %s", repr(inmsg))
+
+            # decode the message
+            inmsg = jsonutils.loads(inmsg)
+
+            s_instance = inmsg.get('source_instance')
+
+            version = inmsg.get('version')
+            if version != GRP_CURRENT_VERSION:
+                log_msg = "invalid guest agent message version %d, " \
+                          "expecting %d" % (version, GRP_CURRENT_VERSION)
+                self.send_server_grp_nack(s_instance, GRP_UNKNOWN, log_msg)
+                continue
+
+            data = inmsg.get(GRP_DATA)
+            if data is None:
+                log_msg = "no valid data found in the message"
+                self.send_server_grp_nack(s_instance, GRP_UNKNOWN, log_msg)
+                continue
+
+            msgtype = data.get(GRP_MSG_TYPE, GRP_UNKNOWN)
+
+            version = data.get(GRP_VERSION)
+            if version != GRP_CURRENT_VERSION:
+                log_msg = "invalid server group message version %d, " \
+                          "expecting %d" % (version, GRP_CURRENT_VERSION)
+                self.send_server_grp_nack(s_instance, msgtype, log_msg)
+                continue
+
+            if msgtype == GRP_BROADCAST:
+                msg_data = data.get(GRP_DATA)
+                nova.utils.spawn_n(self.broadcast_server_grp_msg,
+                                    context, compute_task_api, s_instance,
+                                    msg_data)
+
+            elif msgtype == GRP_STATUS_QUERY:
+                seqnum = data.get(GRP_SEQ)
+                nova.utils.spawn_n(self.server_grp_status_query,
+                                    context, compute_task_api, s_instance,
+                                    seqnum)
+            else:
+                log_msg = "got server group message with invalid type %d" \
+                          % msgtype
+                self.send_server_grp_nack(s_instance, msgtype, log_msg)
+
+    @utils.synchronized(server_grp_name)
+    def send_server_grp_msg(self, instance_name, data):
+        """send message to the guest via the cgcs messaging
+           backchannel daemon
+        """
+        cgcs_msg_address = '\00' + 'cgcs.messaging'
+
+        # add app-to-daemon header onto the message
+        # need to strip trailing NULs off the name otherwise LOG chokes
+        LOG.debug("sending to %s ", instance_name.strip('\x00'))
+
+        outmsg = dict()
+        outmsg[GRP_VERSION] = GRP_CURRENT_VERSION
+        outmsg[GRP_DEST_INSTANCE] = str(instance_name)
+        outmsg[GRP_DEST_ADDR] = server_grp_name
+        outmsg[GRP_DATA] = data
+        outmsg = jsonutils.dumps(outmsg, separators=(',', ':'))
+
+        LOG.debug("sending out to cgcs.messaging %s", outmsg)
+
+        # Send it to the guest via the host agent unix socket
+        self.server_grp_sock.sendto(outmsg, socket.MSG_DONTWAIT,
+                                    cgcs_msg_address)
+
+    @staticmethod
+    def build_notification_msg(data):
+        """Take the given data, prepend it with the header for a server group
+           notification message.
+        """
+        outmsg = dict()
+        outmsg[GRP_VERSION] = GRP_CURRENT_VERSION
+        outmsg[GRP_MSG_TYPE] = GRP_NOTIFICATION
+        outmsg[GRP_DATA] = data
+        return outmsg
+
+
+def send_server_grp_notification(context, event_type, payload, instance_uuid):
+    """Sends a notification to other instances in the same server group"""
+    message = {}
+    message['payload'] = jsonutils.to_primitive(payload,
+                                                convert_instances=True)
+    message['event_type'] = event_type
+    message['priority'] = 'INFO'
+    message['timestamp'] = timeutils.utcnow().isoformat()
+
+    # Do whatever is needed to make a message ready to send
+    message = CGCSMessaging.build_notification_msg(message)
+    LOG.debug("sending notification msg %s", repr(message))
+
+    from nova import conductor
+    task_api = conductor.ComputeTaskAPI()
+    task_api.send_server_group_msg(context, message,
+                                   instance_uuid=instance_uuid)
diff --git a/nova/compute/manager.py b/nova/compute/manager.py
index c57c5d8..048bf65 100644
--- a/nova/compute/manager.py
+++ b/nova/compute/manager.py
@@ -65,6 +65,7 @@ from nova import block_device
 from nova.cells import rpcapi as cells_rpcapi
 from nova import compute
 from nova.compute import build_results
+from nova.compute import cgcs_messaging
 from nova.compute import claims
 from nova.compute import power_state
 from nova.compute import resource_tracker
@@ -569,6 +570,12 @@ class ComputeManager(manager.Manager):
             self._resource_tracker = rt
         return self._resource_tracker
 
+    # server group messaging
+    def send_server_group_msg(self, context, instance_name_list, data):
+        """Forward the data to all the instances in the list """
+        for instance_name in instance_name_list:
+            self.cgcs_messaging.send_server_grp_msg(instance_name, data)
+
     def _update_resource_tracker(self, context, instance):
         """Let the resource tracker know that an instance has changed state."""
 
@@ -1185,6 +1192,10 @@ class ComputeManager(manager.Manager):
             eventlet.semaphore.BoundedSemaphore(
                 CONF.concurrent_disk_operations)
 
+        # add support for server group messaging
+        self.cgcs_messaging = \
+                           cgcs_messaging.CGCSMessaging(self.compute_task_api)
+
         try:
             # Destroy any running domains not in the DB
             self._destroy_orphan_instances(context)
diff --git a/nova/compute/rpcapi.py b/nova/compute/rpcapi.py
index 62d3314..0bdd989 100644
--- a/nova/compute/rpcapi.py
+++ b/nova/compute/rpcapi.py
@@ -11,12 +11,13 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-# Copyright (c) 2016-2017 Wind River Systems, Inc.
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
 #
 # The right to copy, distribute, modify, or otherwise make use
 # of this software may be licensed only pursuant to the terms
 # of an applicable Wind River license agreement.
 #
+
 """
 Client side of the compute RPC API.
 """
@@ -1132,6 +1133,17 @@ class ComputeAPI(object):
                    volume_id=volume_id, snapshot_id=snapshot_id,
                    delete_info=delete_info)
 
+    # add server group messaging, support Kilo compatible version
+    def send_server_group_msg(self, ctxt, host, instance_name_list, data):
+        """Forward the data to all the instances in the list."""
+        LOG.debug('Sending server group message to %s on host %s: %s',
+                  (str(instance_name_list), host, repr(data)))
+        version = '4.0'
+        cctxt = self.router.client(ctxt).prepare(
+                server=host, version=version)
+        cctxt.cast(ctxt, 'send_server_group_msg',
+                   instance_name_list=instance_name_list, data=data)
+
     def external_instance_event(self, ctxt, instances, events, host=None):
         instance = instances[0]
         cctxt = self.router.client(ctxt).prepare(
diff --git a/nova/compute/utils.py b/nova/compute/utils.py
index e5ac0e8..3f439b2 100644
--- a/nova/compute/utils.py
+++ b/nova/compute/utils.py
@@ -11,6 +11,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
+#
+#
+#
+#
+#
 
 """Compute-related Utilities and helpers."""
 
@@ -27,6 +34,7 @@ from oslo_log import log
 import six
 
 from nova import block_device
+from nova.compute import cgcs_messaging
 from nova.compute import power_state
 from nova.compute import task_states
 import nova.conf
@@ -333,6 +341,15 @@ def notify_about_instance_usage(notifier, context, instance, event_suffix,
     else:
         method = notifier.info
 
+    # send server group notifications
+    try:
+        objects.InstanceGroup.get_by_instance_uuid(context, instance.uuid)
+        cgcs_messaging.send_server_grp_notification(
+            context, 'compute.instance.%s' % event_suffix, usage_info,
+            instance.uuid)
+    except exception.InstanceGroupNotFound:
+        pass
+
     method(context, 'compute.instance.%s' % event_suffix, usage_info)
 
 
diff --git a/nova/conductor/api.py b/nova/conductor/api.py
index 947d755..57d4f2b 100644
--- a/nova/conductor/api.py
+++ b/nova/conductor/api.py
@@ -11,6 +11,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
+#
+#
+#
+#
+#
 
 """Handles all requests to the conductor service."""
 
@@ -155,3 +162,16 @@ class ComputeTaskAPI(object):
                 preserve_ephemeral=preserve_ephemeral,
                 host=host,
                 request_spec=request_spec)
+
+    # send server group message
+    def send_server_group_msg(self, context, data, exclude_instance=True,
+                              instance_id=None, instance_uuid=None):
+        """Send message to all instances in the same server group"""
+        return self.conductor_compute_rpcapi.send_server_group_msg(context,
+                            exclude_instance, instance_id, instance_uuid, data)
+
+    # get server group status
+    def get_server_group_status(self, context, instance_id):
+        """Get info on all instances in the same server group"""
+        return self.conductor_compute_rpcapi.get_server_group_status(context,
+                            instance_id)
diff --git a/nova/conductor/manager.py b/nova/conductor/manager.py
index 3dad78f..b368652 100644
--- a/nova/conductor/manager.py
+++ b/nova/conductor/manager.py
@@ -12,7 +12,7 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-# Copyright (c) 2016-2017 Wind River Systems, Inc.
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
 #
 # The right to copy, distribute, modify, or otherwise make use
 # of this software may be licensed only pursuant to the terms
@@ -29,6 +29,7 @@ from oslo_config import cfg
 from oslo_log import log as logging
 import oslo_messaging as messaging
 from oslo_utils import excutils
+from oslo_utils import strutils
 from oslo_utils import versionutils
 import six
 
@@ -1352,3 +1353,115 @@ class ComputeTaskManager(base.Base):
                         pass
             return False
         return True
+
+    # send server group message
+    def send_server_group_msg(self, context, exclude_instance, instance_id,
+                              instance_uuid, data):
+        """Send message to all instances in the same server group
+
+           The sender can specify an instance by either id or uuid.
+           If "exclude_instance" is True then we will not send the message
+           to that instance.
+        """
+        try:
+            # get the instance (could be deleted so use temporary context)
+            deleted_context = copy.deepcopy(context)
+            deleted_context.read_deleted = 'yes'
+            if instance_id is not None:
+                instance = objects.Instance.get_by_id(
+                    deleted_context, instance_id)
+            elif instance_uuid is not None:
+                try:
+                    instance = objects.Instance.get_by_uuid(
+                        deleted_context, instance_uuid)
+                except exception.InstanceNotFound:
+                    # Instance might not have been scheduled
+                    return
+            else:
+                raise exception.InvalidID(instance_id)
+
+            server_group = objects.InstanceGroup.get_by_instance_uuid(
+                context, instance.uuid)
+
+            if not server_group.metadetails.get('wrs-sg:active_listener'):
+                if strutils.bool_from_string(instance.flavor.extra_specs.get(
+                        'sw:wrs:srv_grp_messaging', 'false')):
+                    server_group.metadetails['wrs-sg:active_listener'] = 'true'
+                    server_group._changed_fields.add('metadetails')
+                    server_group.save()
+                else:
+                    return
+
+            # Build a dict where the keys are all compute nodes hosting group
+            # members, with a list of members for each compute node.
+            send_info = {}
+            found_listener = False
+            for uuid in server_group.members:
+                # Check whether we want to exclude this instance
+                if exclude_instance and instance.uuid == uuid:
+                    if strutils.bool_from_string(
+                            instance.flavor.extra_specs.get(
+                                'sw:wrs:srv_grp_messaging', 'false')):
+                        found_listener = True
+                    continue
+                try:
+                    d_instance = objects.Instance.get_by_uuid(context, uuid,
+                        expected_attrs=['flavor'])
+
+                    # skip instances not registered for server group messaging
+                    srv_grp_messaging = strutils.bool_from_string(
+                        d_instance.flavor.extra_specs.get(
+                            'sw:wrs:srv_grp_messaging', 'false'))
+                    if not srv_grp_messaging:
+                        continue
+                    found_listener = True
+                    # Initially send_info is going to be empty, be careful.
+                    send_info[d_instance.host] = [d_instance.name] + \
+                        send_info.get(d_instance.host, [])
+                except exception.InstanceNotFound:
+                    # Instance could have just been deleted, it's fine
+                    pass
+            if not found_listener:
+                server_group.metadetails.pop('wrs-sg:active_listener')
+                server_group._changed_fields.add('metadetails')
+                server_group.save()
+
+            # Call each compute node with message and list of instance names
+            for host in send_info.keys():
+                self.compute_rpcapi.send_server_group_msg(context, host,
+                       send_info[host], data)
+
+        except exception.InstanceGroupNotFound:
+            # server group has been deleted, this is okay
+            pass
+
+    # get server group status
+    def get_server_group_status(self, context, instance_id):
+        """Get status of all instances in the same server group """
+        statuslist = []
+        try:
+            # Get the instance
+            instance = objects.Instance.get_by_id(context, instance_id)
+        except exception.InstanceNotFound:
+            # Instance could have just been deleted, not an error
+            return statuslist
+
+        try:
+            server_group = objects.InstanceGroup.get_by_instance_uuid(
+                context, instance.uuid)
+
+            for uuid in server_group.members:
+                try:
+                    instance = objects.Instance.get_by_uuid(context, uuid)
+                except exception.InstanceNotFound:
+                    # Instance could have just been deleted, not an error
+                    pass
+
+                status = notifications.base.info_from_instance(
+                              context, instance, None, None)
+                statuslist.append(status)
+
+        except exception.InstanceGroupNotFound:
+            # server group has been deleted, this is okay
+            pass
+        return statuslist
diff --git a/nova/conductor/rpcapi.py b/nova/conductor/rpcapi.py
index 5a1a06c..6aacfc9 100644
--- a/nova/conductor/rpcapi.py
+++ b/nova/conductor/rpcapi.py
@@ -12,6 +12,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
+#
+#
+#
+#
+#
 
 """Client side of the conductor RPC API."""
 
@@ -412,3 +419,19 @@ class ComputeTaskAPI(object):
             del kw['request_spec']
         cctxt = self.client.prepare(version=version)
         cctxt.cast(ctxt, 'rebuild_instance', **kw)
+
+    # send server group messaging, same api version (1.6) as R2/Kilo
+    def send_server_group_msg(self, context, exclude_instance, instance_id,
+                              instance_uuid, data):
+        """Send message to all instances in the same server group"""
+        cctxt = self.client.prepare(version='1.6')
+        return cctxt.cast(context, 'send_server_group_msg',
+                          exclude_instance=exclude_instance,
+                          instance_id=instance_id,
+                          instance_uuid=instance_uuid, data=data)
+
+    # get server group status, same api version (1.6) as R2/Kilo
+    def get_server_group_status(self, context, instance_id):
+        cctxt = self.client.prepare(version='1.6')
+        return cctxt.call(context, 'get_server_group_status',
+                          instance_id=instance_id)
diff --git a/nova/notifications/base.py b/nova/notifications/base.py
index 0797bba..b826ba6 100644
--- a/nova/notifications/base.py
+++ b/nova/notifications/base.py
@@ -33,6 +33,7 @@ from oslo_utils import excutils
 from oslo_utils import timeutils
 import six
 
+from nova.compute import cgcs_messaging
 import nova.conf
 import nova.context
 from nova import exception
@@ -260,6 +261,15 @@ def send_instance_update_notification(context, instance, old_vm_state=None,
 
     _send_versioned_instance_update(context, instance, payload, host, service)
 
+    # server group notification
+    try:
+        objects.InstanceGroup.get_by_instance_uuid(context, instance.uuid)
+        cgcs_messaging.send_server_grp_notification(context,
+                                                    'compute.instance.update',
+                                                    payload, instance.uuid)
+    except exception.InstanceGroupNotFound:
+        pass
+
 
 @rpc.if_notifications_enabled
 def _send_versioned_instance_update(context, instance, payload, host, service):
diff --git a/nova/rpc.py b/nova/rpc.py
index 969e2c2..dbe0723 100644
--- a/nova/rpc.py
+++ b/nova/rpc.py
@@ -365,6 +365,7 @@ class LegacyValidatingNotifier(object):
         'servergroup.addmember',
         'servergroup.create',
         'servergroup.delete',
+        'servergroup.update',
         'volume.usage',
     ]
 
diff --git a/nova/tests/unit/api/openstack/compute/test_services.py b/nova/tests/unit/api/openstack/compute/test_services.py
index 4dc2443..ca81369 100644
--- a/nova/tests/unit/api/openstack/compute/test_services.py
+++ b/nova/tests/unit/api/openstack/compute/test_services.py
@@ -966,6 +966,10 @@ class ServicesTestV253(test.TestCase):
         self.req = fakes.HTTPRequest.blank(
             '', version=services_v21.UUID_FOR_ID_MIN_VERSION)
 
+        # stub out server group messaging
+        self.stubs.Set(compute.cgcs_messaging.CGCSMessaging,
+                       '_do_setup', lambda *a, **kw: None)
+
     def assert_services_equal(self, s1, s2):
         for k in ('binary', 'host'):
             self.assertEqual(s1[k], s2[k])
diff --git a/nova/tests/unit/compute/test_compute.py b/nova/tests/unit/compute/test_compute.py
index fb1efc6..3a1d691 100644
--- a/nova/tests/unit/compute/test_compute.py
+++ b/nova/tests/unit/compute/test_compute.py
@@ -16,12 +16,13 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
-# Copyright (c) 2015-2017 Wind River Systems, Inc.
+# Copyright (c) 2013-2017 Wind River Systems, Inc.
 #
 # The right to copy, distribute, modify, or otherwise make use
 # of this software may be licensed only pursuant to the terms
 # of an applicable Wind River license agreement.
 #
+
 """Tests for compute service."""
 
 import contextlib
@@ -1518,6 +1519,9 @@ class ComputeTestCase(BaseTestCase,
     def setUp(self):
         super(ComputeTestCase, self).setUp()
         self.useFixture(fixtures.SpawnIsSynchronousFixture())
+        # stub out server group messaging
+        self.stubs.Set(nova.compute.cgcs_messaging,
+                      'send_server_grp_notification', lambda *a, **kw: None)
 
     def test_wrap_instance_fault(self):
         inst = {"uuid": uuids.instance}
@@ -3401,8 +3405,9 @@ class ComputeTestCase(BaseTestCase,
         instance.save()
         return instance
 
+    @mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid')
     @mock.patch.object(nova.compute.utils, 'notify_about_instance_action')
-    def test_snapshot(self, mock_notify_action):
+    def test_snapshot(self, mock_notify_action, mock_get_sg):
         inst_obj = self._get_snapshotting_instance()
         mock_context = mock.Mock()
         with mock.patch.object(self.context, 'elevated',
@@ -6377,10 +6382,11 @@ class ComputeTestCase(BaseTestCase,
         mock_get_node.return_value = dest_node
         mock_bdms.return_value = objects.BlockDeviceMappingList()
 
+        @mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid')
         @mock.patch('nova.compute.utils.notify_about_instance_action')
         @mock.patch.object(self.compute, '_live_migration_cleanup_flags')
         @mock.patch.object(self.compute, 'network_api')
-        def _test(mock_nw_api, mock_lmcf, mock_notify):
+        def _test(mock_nw_api, mock_lmcf, mock_notify, mock_get_sg):
             mock_lmcf.return_value = False, False
             self.compute._rollback_live_migration(c, instance, 'foo',
                                                   migrate_data=migrate_data)
@@ -6417,10 +6423,11 @@ class ComputeTestCase(BaseTestCase,
         mock_get_node.return_value = dest_node
         mock_bdms.return_value = objects.BlockDeviceMappingList()
 
+        @mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid')
         @mock.patch('nova.compute.utils.notify_about_instance_action')
         @mock.patch.object(self.compute, '_live_migration_cleanup_flags')
         @mock.patch.object(self.compute, 'network_api')
-        def _test(mock_nw_api, mock_lmcf, mock_notify):
+        def _test(mock_nw_api, mock_lmcf, mock_notify, mock_get_sg):
             mock_lmcf.return_value = False, False
             self.compute._rollback_live_migration(c, instance, 'foo',
                                                   migrate_data=migrate_data,
diff --git a/nova/tests/unit/compute/test_compute_mgr.py b/nova/tests/unit/compute/test_compute_mgr.py
index e3eed08..38e3d3b 100644
--- a/nova/tests/unit/compute/test_compute_mgr.py
+++ b/nova/tests/unit/compute/test_compute_mgr.py
@@ -91,6 +91,9 @@ class ComputeManagerUnitTestCase(test.NoDBTestCase):
 
         self.useFixture(fixtures.SpawnIsSynchronousFixture())
         self.useFixture(fixtures.EventReporterStub())
+        # stub out server group messaging
+        self.stubs.Set(nova.compute.cgcs_messaging.CGCSMessaging,
+                      '_do_setup', lambda *a, **kw: None)
 
     @mock.patch.object(manager.ComputeManager, '_get_power_state')
     @mock.patch.object(manager.ComputeManager, '_sync_instance_power_state')
@@ -6141,13 +6144,14 @@ class ComputeManagerMigrationTestCase(test.NoDBTestCase):
         migration.status = 'running'
         migration.id = 0
 
+        @mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid')
         @mock.patch('nova.image.glance.generate_image_url',
                     return_value='fake-url')
         @mock.patch.object(objects.Migration, 'get_by_id',
                            return_value=migration)
         @mock.patch.object(self.compute.driver,
                            'live_migration_force_complete')
-        def _do_test(force_complete, get_by_id, gen_img_url):
+        def _do_test(force_complete, get_by_id, gen_img_url, get_group_by_id):
             self.compute.live_migration_force_complete(
                 self.context, self.instance, migration.id)
 
diff --git a/nova/tests/unit/conductor/test_conductor.py b/nova/tests/unit/conductor/test_conductor.py
index bbe7806..8d3baf8 100644
--- a/nova/tests/unit/conductor/test_conductor.py
+++ b/nova/tests/unit/conductor/test_conductor.py
@@ -32,6 +32,7 @@ from oslo_versionedobjects import exception as ovo_exc
 import six
 
 from nova import block_device
+from nova.compute import cgcs_messaging
 from nova.compute import flavors
 from nova.compute import rpcapi as compute_rpcapi
 from nova.compute import task_states
@@ -1486,6 +1487,10 @@ class ConductorTaskTestCase(_BaseTaskTestCase, test_compute.BaseTestCase):
         params['tags'] = objects.TagList(objects=[tag])
         self.params = params
 
+        # stub out server group messaging
+        self.stubs.Set(cgcs_messaging.CGCSMessaging,
+                       '_do_setup', lambda *a, **kw: None)
+
     @mock.patch('nova.availability_zones.get_host_availability_zone')
     @mock.patch('nova.compute.rpcapi.ComputeAPI.build_and_run_instance')
     @mock.patch('nova.scheduler.rpcapi.SchedulerAPI.select_destinations')
diff --git a/nova/tests/unit/notifications/test_base.py b/nova/tests/unit/notifications/test_base.py
index 8ea3562..31f68b5 100644
--- a/nova/tests/unit/notifications/test_base.py
+++ b/nova/tests/unit/notifications/test_base.py
@@ -15,12 +15,32 @@
 import datetime
 
 import mock
+from oslo_utils import uuidutils
 
+from nova.compute import cgcs_messaging
 from nova.notifications import base
+from nova import objects
 from nova import test
+from nova.tests.unit import fake_instance
 from nova import utils
 
 
+def _get_fake_instance(**kwargs):
+    system_metadata = []
+    for k, v in kwargs.items():
+        system_metadata.append({
+            "key": k,
+            "value": v
+        })
+
+    return {
+        "system_metadata": system_metadata,
+        "uuid": "uuid",
+        "key_data": "ssh-rsa asdf",
+        "os_type": "asdf",
+    }
+
+
 class TestNullSafeUtils(test.NoDBTestCase):
     def test_null_safe_isotime(self):
         dt = None
@@ -59,6 +79,8 @@ class TestSendInstanceUpdateNotification(test.NoDBTestCase):
                                              mock.sentinel.host,
                                              mock.sentinel.service)
 
+    @mock.patch.object(objects.InstanceGroup, 'get_by_instance_uuid')
+    @mock.patch.object(cgcs_messaging, 'send_server_grp_notification')
     @mock.patch.object(base, 'bandwidth_usage')
     @mock.patch.object(base, '_compute_states_payload')
     @mock.patch('nova.rpc.get_notifier')
@@ -66,15 +88,20 @@ class TestSendInstanceUpdateNotification(test.NoDBTestCase):
     def test_send_legacy_instance_update_notification(self, mock_info,
                                                       mock_get_notifier,
                                                       mock_states,
-                                                      mock_bw):
+                                                      mock_bw,
+                                                      mock_send_sg_notif,
+                                                      mock_get_sg):
         """Tests the case that versioned notifications are disabled and
         assert that this does not prevent sending the unversioned
         instance.update notification.
         """
 
         self.flags(notification_format='unversioned', group='notifications')
+
+        instance = fake_instance.fake_instance_obj(
+            mock.sentinel.ctxt, uuid=uuidutils.generate_uuid())
         base.send_instance_update_notification(mock.sentinel.ctxt,
-                                               mock.sentinel.instance)
+                                               instance)
 
         mock_get_notifier.return_value.info.assert_called_once_with(
             mock.sentinel.ctxt, 'compute.instance.update', mock.ANY)
diff --git a/nova/tests/unit/test_baserpc.py b/nova/tests/unit/test_baserpc.py
index 89977e3..d19bba3 100644
--- a/nova/tests/unit/test_baserpc.py
+++ b/nova/tests/unit/test_baserpc.py
@@ -13,11 +13,18 @@
 #    License for the specific language governing permissions and limitations
 #    under the License.
 #
+# Copyright (c) 2016-2017 Wind River Systems, Inc.
+#
+#
+#
+#
+#
 
 """
 Test the base rpc API.
 """
 
+import mock
 from nova import baserpc
 from nova.compute import rpcapi as compute_rpcapi
 import nova.conf
@@ -29,7 +36,9 @@ CONF = nova.conf.CONF
 
 class BaseAPITestCase(test.TestCase):
 
-    def setUp(self):
+    # mock out server group messaging
+    @mock.patch('nova.compute.cgcs_messaging.CGCSMessaging._do_setup')
+    def setUp(self, mock_do_setup):
         super(BaseAPITestCase, self).setUp()
         self.user_id = 'fake'
         self.project_id = 'fake'
diff --git a/nova/tests/unit/test_test.py b/nova/tests/unit/test_test.py
index 70030af..50990a7 100644
--- a/nova/tests/unit/test_test.py
+++ b/nova/tests/unit/test_test.py
@@ -13,9 +13,17 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2016-2017 Wind River Systems, Inc.
+#
+#
+#
+#
+#
 
 """Tests for the testing base code."""
 
+import mock
 from oslo_log import log as logging
 import oslo_messaging as messaging
 import six
@@ -38,7 +46,9 @@ class IsolationTestCase(test.TestCase):
     of other tests should fail.
 
     """
-    def test_service_isolation(self):
+    # mock out server group messaging
+    @mock.patch('nova.compute.cgcs_messaging.CGCSMessaging._do_setup')
+    def test_service_isolation(self, mock_do_setup):
         self.useFixture(fixtures.ServiceFixture('compute'))
 
     def test_rpc_consumer_isolation(self):
diff --git a/nova/tests/unit/virt/test_virt_drivers.py b/nova/tests/unit/virt/test_virt_drivers.py
index c1aeab5..fb6bdc3 100644
--- a/nova/tests/unit/virt/test_virt_drivers.py
+++ b/nova/tests/unit/virt/test_virt_drivers.py
@@ -11,6 +11,13 @@
 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 #    License for the specific language governing permissions and limitations
 #    under the License.
+#
+# Copyright (c) 2016-2017 Wind River Systems, Inc.
+#
+#
+#
+#
+#
 
 import base64
 from collections import deque
@@ -27,6 +34,7 @@ from oslo_utils import importutils
 from oslo_utils import timeutils
 import six
 
+from nova.compute import cgcs_messaging
 from nova.compute import manager
 from nova.compute import power_state
 from nova.console import type as ctype
@@ -205,6 +213,11 @@ class VirtDriverLoaderTestCase(_FakeDriverBackendTestCase, test.TestCase):
         }
 
     def test_load_new_drivers(self):
+        def fake_do_setup(_self, compute_task_api):
+            pass
+
+        self.stubs.Set(cgcs_messaging.CGCSMessaging,
+                       '_do_setup', fake_do_setup)
         for cls, driver in self.new_drivers.items():
             self.flags(compute_driver=cls)
             # NOTE(sdague) the try block is to make it easier to debug a
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 6760728..2e0e763 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -5049,6 +5049,9 @@ class LibvirtDriver(driver.ComputeDriver):
         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
         if virt_type in ('qemu', 'kvm'):
             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
+            self._set_qemu_guest_heartbeat(guest, flavor, instance)
+
+        self._set_server_group_msg(guest, flavor, instance)
 
         self._guest_add_pci_devices(guest, instance)
 
@@ -8503,3 +8506,35 @@ class LibvirtDriver(driver.ComputeDriver):
         except exception.InstanceNotFound:
             # If the instance is already gone, we're happy.
             pass
+
+    def _set_qemu_guest_heartbeat(self, guest, flavor, instance):
+        # Conditionally set up a heartbeat channel
+        heartbeat_value = flavor.extra_specs.get('sw:wrs:guest:heartbeat')
+        do_heartbeat = strutils.bool_from_string(heartbeat_value)
+
+        if do_heartbeat:
+            LOG.info("Guest Heartbeat Enabled", instance=instance)
+            LOG.info('Creating libvirt Heartbeat channel into Guest '
+                     'instance=%(instance)s', {'instance': instance})
+            wrs_hb = vconfig.LibvirtConfigGuestChannel()
+            wrs_hb.type = "unix"
+            wrs_hb.target_name = "cgcs.heartbeat"
+            wrs_hb.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
+                                  ("cgcs.heartbeat", instance['uuid']))
+            guest.add_device(wrs_hb)
+
+        elif heartbeat_value is not None:
+            # Heartbeat explicitly disabled
+            LOG.info("Guest Heartbeat Disabled", instance=instance)
+
+    def _set_server_group_msg(self, guest, flavor, instance):
+        # support server group messaging
+        # Set up virtio channel for cgcs messaging
+        if strutils.bool_from_string(flavor.extra_specs.get(
+                'sw:wrs:srv_grp_messaging', 'false')):
+            wrs_msg = vconfig.LibvirtConfigGuestChannel()
+            wrs_msg.type = "unix"
+            wrs_msg.target_name = "cgcs.messaging"
+            wrs_msg.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
+                                ("cgcs.messaging", instance['name']))
+            guest.add_device(wrs_msg)
-- 
2.7.4

