From e25afb185863aa95ba199223dbf0c083d8f66c13 Mon Sep 17 00:00:00 2001
From: Gerry Kopec <Gerry.Kopec@windriver.com>
Date: Fri, 3 Jun 2016 19:07:17 -0400
Subject: [PATCH 028/143] primary: Add shared vcpu extension

This extension allows multiple low-usage vcpus from different instances
to share a single pcpu in order to put more instances onto a compute
node.

Extension is enabled by flavor extra spec hw:wrs:shared_vcpu to identify
the vcpu of guest that will be pinned to the shared pcpu.  It is only
valid to enable shared cpu if the flavor has enabled dedicated cpus.

NUMACell and InstanceNUMACell objects are modified to support shared cpu.
Shared cpu is passed to nova via shared_pcpu_map config option.
Shared vcpu is not counted in compute node resource impact of an instance.
If also using cpu scaling, offline functions will pin the offlined vcpu
to the pcpu used by the shared vcpu if it exists.

This commit merges the following R3 commits:

740236f Port shared vcpu extension to Mitaka
80edfd4 Port shared vcpu extension to Mitaka - Part 2
c1797e8 Fixed guest with shared vcpu on live migration
635e359 Flavor extra-spec API exception errors

Also includes R4 commit:
f2defde add nova tox tests for shared cpu

fe0899f tweak pinning of emulator threads for realtime and shared_vcpu
   Upstream commit 6683bf9b7 fixed cpu_realtime handling of emulatorpin
   so that they actually map to pCPUs. This change refactors that with
   existing customizations to affine the emulator to the pCPU
   corresponding to vCPU0, or the shared_vcpu if that is defined.

   The tox test_get_guest_config_numa_host_instance_cpu_pinning_realtime
   testcase was updated to properly define cpu_pinning and dedicated
   policy.

e5401b2 vm scaling and shared_vcpu robustness
   This enables the interaction of VM scaling with shared_vcpu.
   We allow specifying the extra-specs hw:wrs:min_vcpus=n in conjunction
   with hw:wrs:shared_vcpu=0.
   Additional validation prevents non-zero values.

__TYPE_primary
__TAG_sharedcpu,resource,instancenumacell
__R4_commit_f8956bc
__R3_commit_097691d
__TC2919,TC2920,TC2921,TC2922,TC2923,TC2993,TC5096,TC5097,TC5098,TC5099
---
 nova/api/openstack/compute/flavors_extraspecs.py |  38 +++-
 nova/compute/resource_tracker.py                 |   2 +-
 nova/conf/compute.py                             |   5 +
 nova/objects/instance_numa_topology.py           |  33 +++-
 nova/objects/numa.py                             |  17 +-
 nova/tests/unit/objects/test_objects.py          |   4 +-
 nova/tests/unit/virt/libvirt/test_driver.py      |  27 ++-
 nova/tests/unit/virt/test_hardware.py            | 228 +++++++++++++++++++++++
 nova/utils.py                                    |   3 +-
 nova/virt/hardware.py                            |  90 ++++++++-
 nova/virt/libvirt/driver.py                      |  58 ++++--
 11 files changed, 461 insertions(+), 44 deletions(-)

diff --git a/nova/api/openstack/compute/flavors_extraspecs.py b/nova/api/openstack/compute/flavors_extraspecs.py
index fd951c1..d1561da 100644
--- a/nova/api/openstack/compute/flavors_extraspecs.py
+++ b/nova/api/openstack/compute/flavors_extraspecs.py
@@ -45,7 +45,6 @@ CPU_POLICY_KEY = 'hw:cpu_policy'
 CPU_SCALING_KEY = 'hw:wrs:min_vcpus'
 SHARED_VCPU_KEY = 'hw:wrs:shared_vcpu'
 
-
 # host numa nodes
 MAX_HOST_NUMA_NODES = 4
 
@@ -304,6 +303,42 @@ class FlavorExtraSpecsController(wsgi.Controller):
                 msg = _("No Compute host was found with vmx enabled.")
                 raise webob.exc.HTTPConflict(explanation=msg)
 
+    # shared vcpu extra spec
+    @staticmethod
+    def _validate_shared_vcpu(flavor):
+        key = SHARED_VCPU_KEY
+        specs = flavor.extra_specs
+        if key in specs:
+            try:
+                value = int(specs[key])
+            except ValueError:
+                msg = _('%s must be an integer') % key
+                raise webob.exc.HTTPBadRequest(explanation=msg)
+            if value < 0:
+                msg = _('%s must be greater than or equal to 0') % key
+                raise webob.exc.HTTPBadRequest(explanation=msg)
+            if value >= flavor.vcpus:
+                msg = _('%(K)s value (%(V)d) must be less than flavor vcpus '
+                        '(%(F)d)') \
+                        % {'K': key, 'V': value, 'F': flavor.vcpus}
+                raise webob.exc.HTTPBadRequest(explanation=msg)
+            if specs.get(CPU_POLICY_KEY) != \
+                    fields.CPUAllocationPolicy.DEDICATED:
+                msg = _('%(K)s is only valid when %(P)s is %(D)s.  Either '
+                       'set extra spec %(P)s to %(D)s or do not set %(K)s.') \
+                        % {'K': key,
+                           'P': CPU_POLICY_KEY,
+                           'D': fields.CPUAllocationPolicy.DEDICATED}
+                raise webob.exc.HTTPConflict(explanation=msg)
+            if (value != 0) and (CPU_SCALING_KEY in specs):
+                msg = _('%(SH)s value (%(V)d) is incompatible '
+                        'with %(SC)s. %(SH)s may only be 0 when %(SC)s '
+                        'is specified.') \
+                        % {'SH': SHARED_VCPU_KEY,
+                           'SC': CPU_SCALING_KEY,
+                           'V': value}
+                raise webob.exc.HTTPConflict(explanation=msg)
+
     # Should this go in the flavor object as part of the save()
     # routine?  If you really need the context to validate something,
     # add it back in to the args.
@@ -311,6 +346,7 @@ class FlavorExtraSpecsController(wsgi.Controller):
         self._validate_vcpu_models(flavor)
         self._validate_cpu_policy(flavor)
         self._validate_cpu_thread_policy(flavor)
+        self._validate_shared_vcpu(flavor)
         self._validate_min_vcpus(flavor)
         self._validate_numa_node(flavor)
         self._validate_sw_keys(flavor)
diff --git a/nova/compute/resource_tracker.py b/nova/compute/resource_tracker.py
index 3d454c7..fb66489 100644
--- a/nova/compute/resource_tracker.py
+++ b/nova/compute/resource_tracker.py
@@ -160,7 +160,7 @@ class ResourceTracker(object):
             hardware.host_topology_and_format_from_host(resources)
 
         for cell in host_numa_topology.cells:
-            if vcpu0_phys in cell.cpuset:
+            if vcpu0_cell.id == cell.id:
                 host_has_threads = (cell.siblings and
                                     len(cell.siblings[0]) > 1)
                 if (host_has_threads and vcpu0_cell.cpu_thread_policy ==
diff --git a/nova/conf/compute.py b/nova/conf/compute.py
index 5638ac2..25be246 100644
--- a/nova/conf/compute.py
+++ b/nova/conf/compute.py
@@ -1144,6 +1144,11 @@ wrs_compute_opts = [
                     'downloads, image format conversions, etc.) that we will'
                     'do in parallel.  If this is set too high then response'
                     'time suffers.'),
+
+    # add shared_pcpu_map
+    cfg.DictOpt('shared_pcpu_map',
+                default={},
+                help='Shared pcpu index per numa node'),
 ]
 
 
diff --git a/nova/objects/instance_numa_topology.py b/nova/objects/instance_numa_topology.py
index f33d77c..bf9729d 100644
--- a/nova/objects/instance_numa_topology.py
+++ b/nova/objects/instance_numa_topology.py
@@ -38,6 +38,7 @@ class InstanceNUMACell(base.NovaObject,
     # Version 1.3: Add cpu_policy and cpu_thread_policy fields
     # Version 1.4: Add cpuset_reserved field
     #              Add physnode
+    #              Add shared_vcpu and shared_pcpu_for_vcpu
     VERSION = '1.4'
 
     def obj_make_compatible(self, primitive, target_version):
@@ -60,6 +61,8 @@ class InstanceNUMACell(base.NovaObject,
         'cpu_topology': obj_fields.ObjectField('VirtCPUTopology',
                                                nullable=True),
         'cpu_pinning_raw': obj_fields.DictOfIntegersField(nullable=True),
+        'shared_vcpu': obj_fields.IntegerField(nullable=True),
+        'shared_pcpu_for_vcpu': obj_fields.IntegerField(nullable=True),
         'cpu_policy': obj_fields.CPUAllocationPolicyField(nullable=True),
         'cpu_thread_policy': obj_fields.CPUThreadAllocationPolicyField(
             nullable=True),
@@ -89,6 +92,12 @@ class InstanceNUMACell(base.NovaObject,
         if 'physnode' not in kwargs:
             self.physnode = None
             self.obj_reset_changes(['physnode'])
+        if 'shared_vcpu' not in kwargs:
+            self.shared_vcpu = None
+            self.obj_reset_changes(['shared_vcpu'])
+        if 'shared_pcpu_for_vcpu' not in kwargs:
+            self.shared_pcpu_for_vcpu = None
+            self.obj_reset_changes(['shared_pcpu_for_vcpu'])
 
     def __len__(self):
         return len(self.cpuset)
@@ -158,6 +167,8 @@ class InstanceNUMACell(base.NovaObject,
     def __str__(self):
         return '  {obj_name} (id: {id})\n' \
                '    cpuset: {cpuset}\n' \
+               '    shared_vcpu: {shared_vcpu}\n' \
+               '    shared_pcpu_for_vcpu: {shared_pcpu_for_vcpu}\n' \
                '    memory: {memory}\n' \
                '    physnode: {physnode}\n' \
                '    pagesize: {pagesize}\n' \
@@ -170,6 +181,8 @@ class InstanceNUMACell(base.NovaObject,
             obj_name=self.obj_name(),
             id=self.id if ('id' in self) else None,
             cpuset=self.cpuset,
+            shared_vcpu=self.shared_vcpu,
+            shared_pcpu_for_vcpu=self.shared_pcpu_for_vcpu,
             memory=self.memory,
             physnode=self.physnode,
             pagesize=self.pagesize,
@@ -186,6 +199,8 @@ class InstanceNUMACell(base.NovaObject,
     def __repr__(self):
         return '{obj_name} (id: {id}) ' \
                'cpuset: {cpuset} ' \
+               'shared_vcpu: {shared_vcpu} ' \
+               'shared_pcpu_for_vcpu: {shared_pcpu_for_vcpu} ' \
                'memory: {memory} ' \
                'physnode: {physnode} ' \
                'pagesize: {pagesize} ' \
@@ -198,6 +213,8 @@ class InstanceNUMACell(base.NovaObject,
             obj_name=self.obj_name(),
             id=self.id if ('id' in self) else None,
             cpuset=self.cpuset,
+            shared_vcpu=self.shared_vcpu,
+            shared_pcpu_for_vcpu=self.shared_pcpu_for_vcpu,
             memory=self.memory,
             physnode=self.physnode,
             pagesize=self.pagesize,
@@ -342,6 +359,8 @@ class InstanceNUMATopology(base.NovaObject,
         for cell in self.cells:
             if vcpu in cell.cpu_pinning.keys():
                 return cell, cell.cpu_pinning[vcpu]
+            if vcpu == cell.shared_vcpu:
+                return cell, cell.shared_pcpu_for_vcpu
         raise KeyError('Unable to find pCPU for vCPU %d' % vcpu)
 
     @property
@@ -350,10 +369,15 @@ class InstanceNUMATopology(base.NovaObject,
         if not self.cpu_pinning_requested:
             return offline_cpuset
         # The offline vCPUs will be pinned the same as vCPU0
-        vcpu0_cell, vcpu0_phys = self.vcpu_to_pcpu(0)
+        # or the shared vcpu index if it is assigned
         for cell in self.cells:
+            online_index = 0
+            if cell.shared_vcpu is not None:
+                online_index = cell.shared_vcpu
+            vcpu0_cell, vcpu0_phys = self.vcpu_to_pcpu(online_index)
             for vcpu in cell.cpuset:
-                if vcpu != 0 and cell.cpu_pinning[vcpu] == vcpu0_phys:
+                if (vcpu != online_index
+                    and cell.cpu_pinning[vcpu] == vcpu0_phys):
                     offline_cpuset |= {vcpu}
         return offline_cpuset
 
@@ -361,8 +385,11 @@ class InstanceNUMATopology(base.NovaObject,
         if not self.cpu_pinning_requested:
             return
         # The offline vCPUs will be pinned the same as vCPU0
-        vcpu0_cell, vcpu0_phys = self.vcpu_to_pcpu(0)
         for cell in self.cells:
+            online_index = 0
+            if cell.shared_vcpu is not None:
+                online_index = cell.shared_vcpu
+            vcpu0_cell, vcpu0_phys = self.vcpu_to_pcpu(online_index)
             for vcpu in cell.cpuset:
                 if vcpu in offline_cpus:
                     cell.pin(vcpu, vcpu0_phys)
diff --git a/nova/objects/numa.py b/nova/objects/numa.py
index 41b5961..483b499 100644
--- a/nova/objects/numa.py
+++ b/nova/objects/numa.py
@@ -47,6 +47,7 @@ class NUMACell(base.NovaObject):
     # Version 1.0: Initial version
     # Version 1.1: Added pinned_cpus and siblings fields
     # Version 1.2: Added mempages field
+    #              Added shared_pcpu
     VERSION = '1.2'
 
     fields = {
@@ -58,8 +59,15 @@ class NUMACell(base.NovaObject):
         'pinned_cpus': fields.SetOfIntegersField(),
         'siblings': fields.ListOfSetsOfIntegersField(),
         'mempages': fields.ListOfObjectsField('NUMAPagesTopology'),
+        'shared_pcpu': fields.IntegerField(default=None, nullable=True),
         }
 
+    def __init__(self, **kwargs):
+        super(NUMACell, self).__init__(**kwargs)
+        if 'shared_pcpu' not in kwargs:
+            self.shared_pcpu = None
+            self.obj_reset_changes(['shared_pcpu'])
+
     def __eq__(self, other):
         return all_things_equal(self, other)
 
@@ -136,6 +144,7 @@ class NUMACell(base.NovaObject):
             'mem': {
                 'total': self.memory,
                 'used': self.memory_usage},
+            'shared_pcpu': self.shared_pcpu,
             'cpu_usage': self.cpu_usage}
 
     @classmethod
@@ -145,10 +154,12 @@ class NUMACell(base.NovaObject):
         cpu_usage = data_dict.get('cpu_usage', 0)
         memory = data_dict.get('mem', {}).get('total', 0)
         memory_usage = data_dict.get('mem', {}).get('used', 0)
+        shared_pcpu = data_dict.get('shared_pcpu')
         cell_id = data_dict.get('id')
         return cls(id=cell_id, cpuset=cpuset, memory=memory,
                    cpu_usage=cpu_usage, memory_usage=memory_usage,
-                   mempages=[], pinned_cpus=set([]), siblings=[])
+                   mempages=[], pinned_cpus=set([]), siblings=[],
+                   shared_pcpu=shared_pcpu)
 
     def can_fit_hugepages(self, pagesize, memory):
         """Returns whether memory can fit into hugepages size
@@ -174,6 +185,7 @@ class NUMACell(base.NovaObject):
                '      used: {used}\n' \
                '    cpu_usage: {cpu_usage}\n' \
                '    siblings: {siblings}\n' \
+               '    shared_pcpu: {shared_pcpu}\n' \
                '    pinned_cpus: {pinned_cpus}\n' \
                '    mempages: {mempages}'.format(
             obj_name=self.obj_name(),
@@ -184,6 +196,7 @@ class NUMACell(base.NovaObject):
             used=self.memory_usage,
             cpu_usage=self.cpu_usage if ('cpu_usage' in self) else None,
             siblings=self.siblings,
+            shared_pcpu=self.shared_pcpu,
             pinned_cpus=hardware.format_cpu_spec(
                 self.pinned_cpus, allow_ranges=False),
             mempages=self.mempages,
@@ -196,6 +209,7 @@ class NUMACell(base.NovaObject):
                'mem: total: {total} used: {used} ' \
                'cpu_usage: {cpu_usage} ' \
                'siblings: {siblings} ' \
+               'shared_pcpu: {shared_pcpu} ' \
                'pinned_cpus: {pinned_cpus} ' \
                'mempages: {mempages}'.format(
             obj_name=self.obj_name(),
@@ -206,6 +220,7 @@ class NUMACell(base.NovaObject):
             used=self.memory_usage,
             cpu_usage=self.cpu_usage if ('cpu_usage' in self) else None,
             siblings=self.siblings,
+            shared_pcpu=self.shared_pcpu,
             pinned_cpus=hardware.format_cpu_spec(
                 self.pinned_cpus, allow_ranges=False),
             mempages=self.mempages,
diff --git a/nova/tests/unit/objects/test_objects.py b/nova/tests/unit/objects/test_objects.py
index cdfb20b..bb43c8a 100644
--- a/nova/tests/unit/objects/test_objects.py
+++ b/nova/tests/unit/objects/test_objects.py
@@ -1122,7 +1122,7 @@ object_data = {
     'InstanceList': '2.4-d2c5723da8c1d08e07cb00160edfd292',
     'InstanceMapping': '1.0-65de80c491f54d19374703c0753c4d47',
     'InstanceMappingList': '1.2-ee638619aa3d8a82a59c0c83bfa64d78',
-    'InstanceNUMACell': '1.4-1953775b0abf5211dfcd30eda8402004',
+    'InstanceNUMACell': '1.4-804334af874da7158126f83cbf959036',
     'InstanceNUMATopology': '1.3-ec0030cb0402a49c96da7051c037082a',
     'InstancePCIRequest': '1.1-b1d75ebc716cb12906d9d513890092bf',
     'InstancePCIRequests': '1.1-65e38083177726d806684cb1cc0136d2',
@@ -1139,7 +1139,7 @@ object_data = {
     'MonitorMetric': '1.1-0fc771d8b3f29946f43a72547ceb07f9',
     'MonitorMetricList': '1.1-15ecf022a68ddbb8c2a6739cfc9f8f5e',
     'NicDiagnostics': '1.0-895e9ad50e0f56d5258585e3e066aea5',
-    'NUMACell': '1.2-74fc993ac5c83005e76e34e8487f1c05',
+    'NUMACell': '1.2-632c5185f7f0533b56df397ae07608fb',
     'NUMAPagesTopology': '1.1-edab9fa2dc43c117a38d600be54b4542',
     'NUMATopology': '1.2-c63fad38be73b6afd04715c9c1b29220',
     'NUMATopologyLimits': '1.0-9463e0edd40f64765ae518a539b9dfd2',
diff --git a/nova/tests/unit/virt/libvirt/test_driver.py b/nova/tests/unit/virt/libvirt/test_driver.py
index db2c73d..da5b6de 100755
--- a/nova/tests/unit/virt/libvirt/test_driver.py
+++ b/nova/tests/unit/virt/libvirt/test_driver.py
@@ -2789,7 +2789,7 @@ class LibvirtConnTestCase(test.NoDBTestCase,
             # Emulator must be pinned to union of cfg.cputune.vcpupin[*].cpuset
             self.assertIsInstance(cfg.cputune.emulatorpin,
                                   vconfig.LibvirtConfigGuestCPUTuneEmulatorPin)
-            self.assertEqual(set([0, 1, 24, 25]),
+            self.assertEqual(set([24]),
                              cfg.cputune.emulatorpin.cpuset)
 
             for i, (instance_cell, numa_cfg_cell) in enumerate(zip(
@@ -2888,9 +2888,13 @@ class LibvirtConnTestCase(test.NoDBTestCase,
             cells=[
                 objects.InstanceNUMACell(
                     id=2, cpuset=set([0, 1]),
+                    cpu_policy = fields.CPUAllocationPolicy.DEDICATED,
+                    cpu_pinning = {0: 4, 1: 5},
                     memory=1024, pagesize=2048),
                 objects.InstanceNUMACell(
                     id=3, cpuset=set([2, 3]),
+                    cpu_policy=fields.CPUAllocationPolicy.DEDICATED,
+                    cpu_pinning={2: 6, 3: 7},
                     memory=1024, pagesize=2048)])
         instance_ref = objects.Instance(**self.test_instance)
         instance_ref.numa_topology = instance_topology
@@ -2960,12 +2964,11 @@ class LibvirtConnTestCase(test.NoDBTestCase,
             self.assertEqual(1, len(cfg.cputune.vcpusched))
             self.assertEqual("fifo", cfg.cputune.vcpusched[0].scheduler)
 
-            # Ensure vCPUs 0-1 are pinned on host CPUs 4-5 and 2-3 are
-            # set on host CPUs 6-7 according the realtime mask ^0-1
-            self.assertEqual(set([4, 5]), cfg.cputune.vcpupin[0].cpuset)
-            self.assertEqual(set([4, 5]), cfg.cputune.vcpupin[1].cpuset)
-            self.assertEqual(set([6, 7]), cfg.cputune.vcpupin[2].cpuset)
-            self.assertEqual(set([6, 7]), cfg.cputune.vcpupin[3].cpuset)
+            # Ensure vCPUs are pinned.
+            self.assertEqual(set([4]), cfg.cputune.vcpupin[0].cpuset)
+            self.assertEqual(set([5]), cfg.cputune.vcpupin[1].cpuset)
+            self.assertEqual(set([6]), cfg.cputune.vcpupin[2].cpuset)
+            self.assertEqual(set([7]), cfg.cputune.vcpupin[3].cpuset)
 
             # We ensure that emulator threads are pinned on host CPUs
             # 4-5 which are "normal" vCPUs
@@ -8702,7 +8705,8 @@ class LibvirtConnTestCase(test.NoDBTestCase,
     <vcpupin vcpu="1" cpuset="1"/>
     <vcpupin vcpu="2" cpuset="2"/>
     <vcpupin vcpu="3" cpuset="3"/>
-    <emulatorpin cpuset="1-3"/>
+    <!-- change, emulator is pinned with vCPU 0 -->
+    <emulatorpin cpuset="0"/>
   </cputune>
   <numatune>
     <!-- we no longer emit numatune memory mode
@@ -8748,7 +8752,8 @@ class LibvirtConnTestCase(test.NoDBTestCase,
   <memory unit="KiB">4194304</memory>
   <vcpu>4</vcpu>
   <cputune>
-    <emulatorpin cpuset="4-7"/>
+    <!-- change, emulator is pinned with vCPU 0 -->
+    <emulatorpin cpuset="4"/>
     <vcpupin vcpu="0" cpuset="4"/>
     <vcpupin vcpu="1" cpuset="5"/>
     <vcpupin vcpu="2" cpuset="6"/>
@@ -13543,15 +13548,19 @@ class LibvirtConnTestCase(test.NoDBTestCase,
         expected_topo_dict = {'cells': [
                                 {'cpus': '0,1', 'cpu_usage': 0,
                                   'mem': {'total': 256, 'used': 0},
+                                  'shared_pcpu': None,
                                   'id': 0},
                                 {'cpus': '3', 'cpu_usage': 0,
                                   'mem': {'total': 256, 'used': 0},
+                                  'shared_pcpu': None,
                                   'id': 1},
                                 {'cpus': '', 'cpu_usage': 0,
                                   'mem': {'total': 256, 'used': 0},
+                                  'shared_pcpu': None,
                                   'id': 2},
                                 {'cpus': '', 'cpu_usage': 0,
                                   'mem': {'total': 256, 'used': 0},
+                                  'shared_pcpu': None,
                                   'id': 3}]}
         with test.nested(
                 mock.patch.object(host.Host, "get_capabilities",
diff --git a/nova/tests/unit/virt/test_hardware.py b/nova/tests/unit/virt/test_hardware.py
index d117fcb..9efb2ab 100644
--- a/nova/tests/unit/virt/test_hardware.py
+++ b/nova/tests/unit/virt/test_hardware.py
@@ -1454,6 +1454,73 @@ class NUMATopologyTest(test.NoDBTestCase):
                 },
                 "expect": exception.ImageNUMATopologyNodesForbidden,
             },
+            {
+                # NUMA + CPU pinning requested in flavor + shared_vcpu=0
+                "flavor": objects.Flavor(vcpus=4, memory_mb=2048,
+                                         extra_specs={
+                        "hw:cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
+                        "hw:wrs:shared_vcpu": 0
+                }),
+                "image": {
+                    "properties": {}
+                },
+                "expect": objects.InstanceNUMATopology(cells=
+                    [
+                        objects.InstanceNUMACell(
+                            id=0,
+                            cpuset=set([1, 2, 3]),
+                            memory=2048,
+                            shared_vcpu=0,
+                            cpu_policy=fields.CPUAllocationPolicy.DEDICATED)
+                    ])
+            },
+            {
+                # NUMA + CPU pinning requested in flavor + shared_vcpu=1
+                "flavor": objects.Flavor(vcpus=4, memory_mb=2048,
+                                         extra_specs={
+                        "hw:cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
+                        "hw:wrs:shared_vcpu": 1
+                }),
+                "image": {
+                    "properties": {}
+                },
+                "expect": objects.InstanceNUMATopology(cells=
+                    [
+                        objects.InstanceNUMACell(
+                            id=0,
+                            cpuset=set([0, 2, 3]),
+                            memory=2048,
+                            shared_vcpu=1,
+                            cpu_policy=fields.CPUAllocationPolicy.DEDICATED)
+                    ])
+            },
+            {
+                # For multiple cells, we add shared_vcpu to only one
+                "flavor": objects.Flavor(vcpus=8, memory_mb=2048,
+                                         extra_specs={
+                        "hw:cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
+                        "hw:wrs:shared_vcpu": 1,
+                        "hw:numa_nodes": 2,
+                }),
+                "image": {
+                    "properties": {}
+                },
+                "expect": objects.InstanceNUMATopology(cells=
+                [
+                    objects.InstanceNUMACell(
+                        id=0,
+                        cpuset=set([0, 2, 3]),
+                        memory=1024,
+                        shared_vcpu=1,
+                        cpu_policy=fields.CPUAllocationPolicy.DEDICATED),
+                    objects.InstanceNUMACell(
+                        id=1,
+                        cpuset=set([4, 5, 6, 7]),
+                        memory=1024,
+                        shared_vcpu=None,
+                        cpu_policy=fields.CPUAllocationPolicy.DEDICATED)
+                ])
+            },
         ]
 
         for testitem in testdata:
@@ -1493,6 +1560,11 @@ class NUMATopologyTest(test.NoDBTestCase):
                                      topology.cells[i].physnode)
                     self.assertEqual(testitem["expect"].cells[i].cpu_policy,
                                      topology.cells[i].cpu_policy)
+                    self.assertEqual(testitem["expect"].cells[i].shared_vcpu,
+                                     topology.cells[i].shared_vcpu)
+                    self.assertEqual(
+                        testitem["expect"].cells[i].shared_pcpu_for_vcpu,
+                        topology.cells[i].shared_pcpu_for_vcpu)
 
     def test_host_usage_contiguous(self):
         hpages0_4K = objects.NUMAPagesTopology(size_kb=4, total=256, used=0)
@@ -2828,6 +2900,109 @@ class CPUPinningCellTestCase(test.NoDBTestCase, _CPUPinningTestCaseBase):
         self.assertInstanceCellPinned(inst_pin)
         got_topo = objects.VirtCPUTopology(sockets=1, cores=2, threads=1)
         self.assertEqualTopology(got_topo, inst_pin.cpu_topology)
+        got_pinning = {0: 2, 1: 3}
+        self.assertEqual(got_pinning, inst_pin.cpu_pinning)
+
+    # instance with shared_vcpu but share not enabled on host cell
+    def test_get_pinning_shared_vcpu_mismatch(self):
+        host_pin = objects.NUMACell(id=0, cpuset=set([0, 1, 2]),
+                                    memory=2048, memory_usage=0,
+                                    cpu_usage=0, siblings=[],
+                                    mempages=[], pinned_cpus=set([]),
+                                    shared_pcpu=None)
+        inst_pin = objects.InstanceNUMACell(id=0, cpuset=set([0, 1, 2]),
+                                            memory=2048,
+                                            shared_vcpu=0)
+        inst_pin = hw._numa_fit_instance_cell_with_pinning(host_pin, inst_pin)
+        self.assertIsNone(inst_pin)
+
+    # Disallow instance with only shared cpu on a numa node
+    def test_get_pinning_instance_with_shared_vcpu_only(self):
+        host_pin = objects.NUMACell(id=0, cpuset=set([1, 2]),
+                                    memory=2048, memory_usage=0,
+                                    cpu_usage=0, siblings=[],
+                                    mempages=[], pinned_cpus=set([]),
+                                    shared_pcpu=0)
+        inst_pin = objects.InstanceNUMACell(id=0, cpuset=set([0]),
+                                            memory=2048,
+                                            shared_vcpu=0)
+        inst_pin = hw._numa_fit_instance_cell_with_pinning(host_pin, inst_pin)
+        self.assertIsNone(inst_pin)
+
+    # Instance with shared_vcpu does not fit into host cell
+    def test_get_pinning_shared_vcpu_not_fit(self):
+        host_pin = objects.NUMACell(id=0, cpuset=set([1, 2]),
+                                    memory=2048, memory_usage=0,
+                                    cpu_usage=0, siblings=[],
+                                    mempages=[], pinned_cpus=set([]),
+                                    shared_pcpu=0)
+        inst_pin = objects.InstanceNUMACell(id=0, cpuset=set([0, 1, 2, 3]),
+                                            memory=2048,
+                                            shared_vcpu=0)
+
+        inst_pin = hw._numa_fit_instance_cell_with_pinning(host_pin, inst_pin)
+        self.assertIsNone(inst_pin)
+
+    # instance with shared_vcpu fits into host cell
+    def test_get_pinning_shared_vcpu_fit(self):
+        host_pin = objects.NUMACell(id=0, cpuset=set([0, 2, 3]),
+                                    memory=2048, memory_usage=0,
+                                    cpu_usage=0, siblings=[],
+                                    mempages=[], pinned_cpus=set([]),
+                                    shared_pcpu=1)
+        inst_pin = objects.InstanceNUMACell(id=0, cpuset=set([0, 1, 2, 3]),
+                                            memory=2048,
+                                            shared_vcpu=0)
+
+        inst_pin = hw._numa_fit_instance_cell_with_pinning(host_pin, inst_pin)
+        self.assertInstanceCellPinned(inst_pin)
+        got_topo = objects.VirtCPUTopology(sockets=1, cores=3, threads=1)
+        self.assertEqualTopology(got_topo, inst_pin.cpu_topology)
+        got_pinning = {1: 0, 2: 2, 3: 3}
+        self.assertEqual(got_pinning, inst_pin.cpu_pinning)
+        self.assertEqual(1, inst_pin.shared_pcpu_for_vcpu)
+
+    # instance has no shared_vcpu fits into host cell with shared_pcpu
+    def test_get_pinning_fit_with_shared_pcpu(self):
+        host_pin = objects.NUMACell(id=0, cpuset=set([0, 2, 3]),
+                                    memory=2048, memory_usage=0,
+                                    cpu_usage=0, siblings=[],
+                                    mempages=[], pinned_cpus=set([]),
+                                    shared_pcpu=1)
+        inst_pin = objects.InstanceNUMACell(id=0, cpuset=set([0, 1, 2]),
+                                            memory=2048)
+
+        inst_pin = hw._numa_fit_instance_cell_with_pinning(host_pin, inst_pin)
+        self.assertInstanceCellPinned(inst_pin)
+        got_topo = objects.VirtCPUTopology(sockets=1, cores=3, threads=1)
+        self.assertEqualTopology(got_topo, inst_pin.cpu_topology)
+        got_pinning = {0: 0, 1: 2, 2: 3}
+        self.assertEqual(got_pinning, inst_pin.cpu_pinning)
+        self.assertEqual(1, inst_pin.shared_pcpu_for_vcpu)
+
+    # instance with shared_vcpu fits into hyperthread host
+    def test_get_pinning_shared_vcpu_prefer_policy_fit_w_usage(self):
+        host_pin = objects.NUMACell(
+                id=0,
+                cpuset=set([0, 2, 3, 4, 5, 6, 7]),
+                memory=4096, memory_usage=0, cpu_usage=0,
+                pinned_cpus=set([0, 2]),
+                siblings=[set([0, 4]), set([1, 5]), set([2, 6]), set([3, 7])],
+                mempages=[],
+                shared_pcpu=1)
+        inst_pin = objects.InstanceNUMACell(
+                cpuset=set([0, 1, 2, 3]),
+                memory=2048,
+                shared_vcpu=0,
+                cpu_policy=fields.CPUAllocationPolicy.DEDICATED,
+                cpu_thread_policy=fields.CPUThreadAllocationPolicy.PREFER)
+        inst_pin = hw._numa_fit_instance_cell_with_pinning(host_pin, inst_pin)
+        self.assertInstanceCellPinned(inst_pin)
+        got_topo = objects.VirtCPUTopology(sockets=1, cores=3, threads=1)
+        self.assertEqualTopology(got_topo, inst_pin.cpu_topology)
+        got_pinning = {1: 4, 2: 5, 3: 6}
+        self.assertEqual(got_pinning, inst_pin.cpu_pinning)
+        self.assertEqual(1, inst_pin.shared_pcpu_for_vcpu)
 
 
 class CPUPinningTestCase(test.NoDBTestCase, _CPUPinningTestCaseBase):
@@ -3122,6 +3297,59 @@ class CPUPinningTestCase(test.NoDBTestCase, _CPUPinningTestCaseBase):
         self.assertEqual(set([3]), new_cell.cells[0].pinned_cpus)
         self.assertEqual(new_cell.cells[0].cpu_usage, 1)
 
+    # Instance with shared_vcpu fit
+    def test_host_numa_fit_instance_shared_vcpu_to_host_fit(self):
+        host_topo = objects.NUMATopology(
+                cells=[objects.NUMACell(id=0, cpuset=set([0, 2, 3]),
+                                        memory=2048, memory_usage=0,
+                                        cpu_usage=0,
+                                        siblings=[], mempages=[],
+                                        pinned_cpus=set([]),
+                                        shared_pcpu=1),
+                       objects.NUMACell(id=1, cpuset=set([4, 5, 6, 7]),
+                                        memory=2048, memory_usage=0,
+                                        cpu_usage=0,
+                                        siblings=[], mempages=[],
+                                        pinned_cpus=set([]))])
+        inst_topo = objects.InstanceNUMATopology(
+                cells=[objects.InstanceNUMACell(
+                            cpuset=set([0, 1]), memory=2048,
+                            shared_vcpu=0,
+                            cpu_policy=fields.CPUAllocationPolicy.DEDICATED),
+                       objects.InstanceNUMACell(
+                            cpuset=set([2, 3]), memory=2048,
+                            cpu_policy=fields.CPUAllocationPolicy.DEDICATED)])
+        inst_topo = hw.numa_fit_instance_to_host(host_topo, inst_topo)
+        for cell in inst_topo.cells:
+            self.assertInstanceCellPinned(cell, cell_ids=(0, 1))
+        self.assertEqual({1: 0}, inst_topo.cells[0].cpu_pinning)
+        self.assertEqual({2: 4, 3: 5}, inst_topo.cells[1].cpu_pinning)
+
+    # Multiple instance with shared_vcpu fit to the same shared_pcpu
+    def test_host_numa_fit_multi_instance_shared_vcpu_to_host_fit(self):
+        host_pin = objects.NUMATopology(
+                cells=[objects.NUMACell(id=0, cpuset=set([0, 1, 2, 3]),
+                                        memory=4096, cpu_usage=0,
+                                        memory_usage=0, siblings=[],
+                                        shared_pcpu=1,
+                                        mempages=[], pinned_cpus=set([]))])
+        inst_pin_1 = objects.InstanceNUMATopology(
+                cells=[objects.InstanceNUMACell(
+                    cpuset=set([0, 1]), id=0, memory=2048,
+                    cpu_pinning={1: 0},
+                    shared_vcpu=0,
+                    cpu_policy=fields.CPUAllocationPolicy.DEDICATED)])
+        inst_pin_2 = objects.InstanceNUMATopology(
+                cells = [objects.InstanceNUMACell(
+                    cpuset=set([0, 1]), id=0, memory=2048,
+                    cpu_pinning={1: 3},
+                    shared_vcpu=0,
+                    cpu_policy=fields.CPUAllocationPolicy.DEDICATED)])
+
+        host_pin = hw.numa_usage_from_instances(
+                host_pin, [inst_pin_1, inst_pin_2])
+        self.assertEqual(set([0, 3]), host_pin.cells[0].pinned_cpus)
+
 
 class CPUSReservedCellTestCase(test.NoDBTestCase):
     def _test_reserved(self, reserved):
diff --git a/nova/utils.py b/nova/utils.py
index c32d434..792f6d8 100644
--- a/nova/utils.py
+++ b/nova/utils.py
@@ -211,7 +211,8 @@ def format_instance_numa_topology(numa_topology=None, instance=None,
 
         try:
             if cell.shared_vcpu is not None:
-                cell_str += ', shared_vcpu:%s' % (cell.shared_vcpu)
+                cell_str += ', shared_pcpu:%s' \
+                            % (cell.shared_pcpu_for_vcpu)
         except Exception:
             pass
 
diff --git a/nova/virt/hardware.py b/nova/virt/hardware.py
index ae9e144..6f551ce 100644
--- a/nova/virt/hardware.py
+++ b/nova/virt/hardware.py
@@ -45,6 +45,22 @@ MEMPAGES_LARGE = -2
 MEMPAGES_ANY = -3
 
 
+# shared pcpu extension
+def get_shared_pcpu_map():
+    """Parsing shared_pcpu_map config.
+
+    Returns a map of numa nodes to shared pcpu indices
+    """
+    if not CONF.shared_pcpu_map:
+        return {}
+    shared_pcpu_map = CONF.shared_pcpu_map
+    # Clean out invalid entries
+    for k, v in shared_pcpu_map.items():
+        if not v.isdigit():
+            del shared_pcpu_map[k]
+    return shared_pcpu_map
+
+
 def get_vcpu_pin_set():
     """Parse vcpu_pin_set config.
 
@@ -587,7 +603,11 @@ def _get_desirable_cpu_topologies(flavor, image_meta, allow_threads=True,
             if preferred.threads != -1:
                 min_requested_threads = min(preferred.threads,
                                             min_requested_threads)
-
+            # If flavor is using shared_vcpu, add an extra thread
+            shared_vcpu = flavor.get('extra_specs',
+                                     {}).get("hw:wrs:shared_vcpu", None)
+            if shared_vcpu is not None:
+                min_requested_threads = min_requested_threads + 1
             specified_threads = max(1, min_requested_threads)
             LOG.debug("Filtering topologies best for %d threads",
                       specified_threads)
@@ -656,6 +676,7 @@ def _numa_cell_supports_pagesize_request(host_cell, inst_cell):
 def _pack_instance_onto_cores(available_siblings,
                               instance_cell,
                               host_cell_id,
+                              host_cell_shared_pcpu,
                               threads_per_core=1,
                               num_cpu_reserved=0):
     """Pack an instance onto a set of siblings.
@@ -903,6 +924,8 @@ def _pack_instance_onto_cores(available_siblings,
     instance_cell.cpu_topology = topology
     instance_cell.id = host_cell_id
     instance_cell.cpuset_reserved = cpuset_reserved
+    # add shared pcpu to cell
+    instance_cell.shared_pcpu_for_vcpu = host_cell_shared_pcpu
     return instance_cell
 
 
@@ -919,6 +942,37 @@ def _numa_fit_instance_cell_with_pinning(host_cell, instance_cell,
     :returns: objects.InstanceNUMACell instance with pinning information,
               or None if instance cannot be pinned to the given host
     """
+
+    # exclude cells based on shared_pcpu mismatch
+    if (instance_cell.shared_vcpu is not None and
+        host_cell.shared_pcpu is None):
+        msg = "Shared not enabled for cell %d" % (instance_cell.id,)
+        LOG.debug(msg)
+        return
+
+    # exclude shared_vcpu from cpuset if it exists
+    instance_cell.cpuset.discard(instance_cell.shared_vcpu)
+
+    # Check for an empty cpuset after removing the shared_vcpu
+    if instance_cell.shared_vcpu is not None and not instance_cell.cpuset:
+        msg = ("(numa:%(id)d shared vcpu with 0 requested "
+              "dedicated vcpus is not allowed, avail:%(aval)d)" %
+              {'id': host_cell.id,
+              'aval': len(host_cell.cpuset)})
+        LOG.debug(msg)
+        return
+
+    if host_cell.avail_cpus < len(instance_cell.cpuset):
+        LOG.debug('Not enough available CPUs to schedule instance. '
+                  'Oversubscription is not possible with pinned instances. '
+                  'Required: %(required)s, actual: %(actual)s',
+                  {'required': len(instance_cell.cpuset),
+                   'actual': host_cell.avail_cpus})
+        msg = "NUMA %d: CPUs avail(%d) < required(%d)" \
+              % (host_cell.id, host_cell.avail_cpus, len(instance_cell.cpuset))
+        LOG.debug(msg)
+        return
+
     required_cpus = len(instance_cell.cpuset) + num_cpu_reserved
     if host_cell.avail_cpus < required_cpus:
         LOG.debug('Not enough available CPUs to schedule instance. '
@@ -937,6 +991,9 @@ def _numa_fit_instance_cell_with_pinning(host_cell, instance_cell,
                   'Required: %(required)s, actual: %(actual)s',
                   {'required': instance_cell.memory,
                    'actual': host_cell.memory})
+        msg = "Memory avail(%d) < requested(%d)" \
+              % (host_cell.avail_memory, instance_cell.memory)
+        LOG.debug(msg)
         return
 
     if host_cell.siblings:
@@ -944,6 +1001,7 @@ def _numa_fit_instance_cell_with_pinning(host_cell, instance_cell,
         # Try to pack the instance cell onto cores
         numa_cell = _pack_instance_onto_cores(
             host_cell.free_siblings, instance_cell, host_cell.id,
+            host_cell.shared_pcpu,
             max(map(len, host_cell.siblings)),
             num_cpu_reserved=num_cpu_reserved)
     else:
@@ -958,7 +1016,7 @@ def _numa_fit_instance_cell_with_pinning(host_cell, instance_cell,
         # hyperthreading on the host
         free_cpus = [set([cpu]) for cpu in host_cell.free_cpus]
         numa_cell = _pack_instance_onto_cores(
-            free_cpus, instance_cell, host_cell.id,
+            free_cpus, instance_cell, host_cell.id, host_cell.shared_pcpu,
             num_cpu_reserved=num_cpu_reserved)
 
     if not numa_cell:
@@ -1013,7 +1071,7 @@ def _numa_fit_instance_cell(host_cell, instance_cell, limit_cell=None,
         new_instance_cell = _numa_fit_instance_cell_with_pinning(
             host_cell, instance_cell, cpuset_reserved)
         if not new_instance_cell:
-            return
+            return None
         new_instance_cell.pagesize = instance_cell.pagesize
         instance_cell = new_instance_cell
 
@@ -1044,7 +1102,7 @@ def _numa_fit_instance_cell(host_cell, instance_cell, limit_cell=None,
         if not pagesize:
             LOG.debug('Host does not support requested memory pagesize. '
                       'Requested: %d kB', instance_cell.pagesize)
-            return
+            return None
 
     instance_cell.id = host_cell.id
     instance_cell.pagesize = pagesize
@@ -1546,14 +1604,29 @@ def numa_get_constraints(flavor, image_meta):
             raise exception.CPUThreadPolicyConfigurationInvalid()
         return numa_topology
 
+    # If cpu pinning is requested we can check for hw:wrs:shared_vcpu
+    # This means we "un-pin" the shared_vcpu index
+    shared_vcpu = flavor.get('extra_specs', {}).get("hw:wrs:shared_vcpu", None)
+    if shared_vcpu is not None:
+        shared_vcpu = int(shared_vcpu)
+
     if numa_topology:
         for cell in numa_topology.cells:
             cell.cpu_policy = cpu_policy
             cell.cpu_thread_policy = cpu_thread_policy
+            # For multiple cells, we add shared_vcpu to only one
+            if shared_vcpu in cell.cpuset:
+                cell.cpuset.discard(shared_vcpu)
+                cell.shared_vcpu = shared_vcpu
+            else:
+                cell.shared_vcpu = None
     else:
+        cpu_set = set(range(flavor.vcpus))
+        cpu_set.discard(shared_vcpu)
         single_cell = objects.InstanceNUMACell(
             id=0,
-            cpuset=set(range(flavor.vcpus)),
+            cpuset=cpu_set,
+            shared_vcpu=shared_vcpu,
             memory=flavor.memory_mb,
             cpu_policy=cpu_policy,
             cpu_thread_policy=cpu_thread_policy)
@@ -1716,10 +1789,12 @@ def numa_usage_from_instances(host, instances, free=False):
         memory_usage = hostcell.memory_usage
         cpu_usage = hostcell.cpu_usage
 
+        # add shared_pcpu
         newcell = objects.NUMACell(
             id=hostcell.id, cpuset=hostcell.cpuset, memory=hostcell.memory,
             cpu_usage=0, memory_usage=0, mempages=hostcell.mempages,
-            pinned_cpus=hostcell.pinned_cpus, siblings=hostcell.siblings)
+            pinned_cpus=hostcell.pinned_cpus, siblings=hostcell.siblings,
+            shared_pcpu=hostcell.shared_pcpu)
 
         for instance in instances:
             for cellid, instancecell in enumerate(instance.cells):
@@ -1836,6 +1911,9 @@ def instance_topology_from_instance(instance):
                     cpu_topology=cell.get('cpu_topology'),
                     # add physnode
                     physnode=cell.get('physnode'),
+                    # add shared_vcpu & shared_pcpu_for_vcpu
+                    shared_vcpu=cell.get('shared_vcpu'),
+                    shared_pcpu_for_vcpu=cell.get('shared_pcpu_for_vcpu'),
                     cpu_pinning=cell.get('cpu_pinning_raw'),
                     cpu_policy=cell.get('cpu_policy'),
                     cpu_thread_policy=cell.get('cpu_thread_policy'),
diff --git a/nova/virt/libvirt/driver.py b/nova/virt/libvirt/driver.py
index 5238f1a..1f3b6e6 100644
--- a/nova/virt/libvirt/driver.py
+++ b/nova/virt/libvirt/driver.py
@@ -4257,6 +4257,9 @@ class LibvirtDriver(driver.ComputeDriver):
                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
                 guest_cell.id = instance_cell.id
                 guest_cell.cpus = instance_cell.cpuset
+                # add shared_vcpu if exists
+                if instance_cell.shared_vcpu is not None:
+                    guest_cell.cpus.add(instance_cell.shared_vcpu)
                 guest_cell.memory = instance_cell.memory * units.Ki
 
                 # The vhost-user network backend requires file backed
@@ -4398,7 +4401,6 @@ class LibvirtDriver(driver.ComputeDriver):
                             node.cellid = guest_node_id
                             node.nodeset = [host_cell.id]
                             node.mode = "strict"
-
                             numa_mem.nodeset.append(host_cell.id)
 
                             object_numa_cell = (
@@ -4414,27 +4416,37 @@ class LibvirtDriver(driver.ComputeDriver):
 
                                 if (object_numa_cell.cpu_pinning and
                                         self._has_cpu_policy_support()):
-                                    pcpu = object_numa_cell.cpu_pinning[cpu]
-                                    pin_cpuset.cpuset = set([pcpu])
+                                    # handle shared_vcpu
+                                    if cpu == object_numa_cell.shared_vcpu:
+                                        numa_cell = object_numa_cell
+                                        pin_cpuset.cpuset = set(
+                                            [numa_cell.shared_pcpu_for_vcpu])
+                                    else:
+                                        pcpu = \
+                                            object_numa_cell.cpu_pinning[cpu]
+                                        pin_cpuset.cpuset = set([pcpu])
+                                    # refactor upstream commit 6683bf9b7
+                                    # to work with custom emulator affine
+                                    # and shared_vcpu extension
+                                    if emulator_threads_isolated:
+                                        emupcpus.extend(
+                                            object_numa_cell.cpuset_reserved)
+                                    else:
+                                        if wants_realtime and \
+                                                        cpu not in vcpus_rt:
+                                            emupcpus.extend(pin_cpuset.cpuset)
+                                        # customization: affine the
+                                        # emulator with the pCPU corresponding
+                                        # to vCPU0 or the shared_vcpu if it is
+                                        # defined.
+                                        if not wants_realtime and (
+                                            (object_numa_cell.shared_vcpu
+                                            is None and cpu == 0) or (
+                                            cpu ==
+                                                object_numa_cell.shared_vcpu)):
+                                            emupcpus.extend(pin_cpuset.cpuset)
                                 else:
                                     pin_cpuset.cpuset = host_cell.cpuset
-                                if emulator_threads_isolated:
-                                    emupcpus.extend(
-                                        object_numa_cell.cpuset_reserved)
-                                elif not wants_realtime or cpu not in vcpus_rt:
-                                    # - If realtime IS NOT enabled, the
-                                    #   emulator threads are allowed to float
-                                    #   across all the pCPUs associated with
-                                    #   the guest vCPUs ("not wants_realtime"
-                                    #   is true, so we add all pcpus)
-                                    # - If realtime IS enabled, then at least
-                                    #   1 vCPU is required to be set aside for
-                                    #   non-realtime usage. The emulator
-                                    #   threads are allowed to float acros the
-                                    #   pCPUs that are associated with the
-                                    #   non-realtime VCPUs (the "cpu not in
-                                    #   vcpu_rt" check deals with this
-                                    #   filtering)
                                     emupcpus.extend(pin_cpuset.cpuset)
                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
 
@@ -5902,6 +5914,7 @@ class LibvirtDriver(driver.ComputeDriver):
             allowed_cpus &= online_cpus
         else:
             allowed_cpus = online_cpus
+        sh_pcpu_map = hardware.get_shared_pcpu_map()
 
         def _get_reserved_memory_for_cell(self, cell_id, page_size):
             cell = self._reserved_hugepages.get(cell_id, {})
@@ -5928,11 +5941,16 @@ class LibvirtDriver(driver.ComputeDriver):
                         self, cell.id, pages.size))
                 for pages in cell.mempages]
 
+            # query shared vcpu index from config per numa
+            share_key = str(cell.id)
+            shared_pcpu = sh_pcpu_map.get(share_key, None)
+
             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
                                     memory=cell.memory / units.Ki,
                                     cpu_usage=0, memory_usage=0,
                                     siblings=siblings,
                                     pinned_cpus=set([]),
+                                    shared_pcpu=shared_pcpu,
                                     mempages=mempages)
             cells.append(cell)
 
-- 
2.7.4

