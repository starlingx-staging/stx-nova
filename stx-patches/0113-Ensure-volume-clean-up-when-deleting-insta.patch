From bbbcbb8d111ae1fcdcd5066663fc19a5217ce5a1 Mon Sep 17 00:00:00 2001
From: Stefan Dinescu <stefan.dinescu@windriver.com>
Date: Fri, 2 Mar 2018 17:03:23 +0200
Subject: [PATCH 113/143] Ensure volume clean-up when deleting
 instances in an "error" state

Changes based on upstream commits 1e59ed99c794e6554b9a3e1e2ed197de75e14189
and e5a055d2c6f04330cb83b33f8ddd4c9875fdb1b7

This commit contains just a subset of the changes present in the above
commits, due to merge conflicts as our code is behing the upstream code.

This commit should be removed during a rebase that contains the mentioned
upstream commits.
---
 nova/compute/api.py                         | 65 +++++++++++++++++++++++------
 nova/tests/unit/compute/test_compute_api.py |  6 +--
 2 files changed, 55 insertions(+), 16 deletions(-)

diff --git a/nova/compute/api.py b/nova/compute/api.py
index 6990c64..5c3f493 100644
--- a/nova/compute/api.py
+++ b/nova/compute/api.py
@@ -1502,6 +1502,13 @@ class API(base.Base):
                         volume = self._check_attach(context, volume_id,
                                                     instance)
                     bdm.volume_size = volume.get('size')
+
+                    # NOTE(mnaser): If we end up reserving the volume, it will
+                    #               not have an attachment_id which is needed
+                    #               for cleanups.  This can be removed once
+                    #               all calls to reserve_volume are gone.
+                    if 'attachment_id' not in bdm:
+                        bdm.attachment_id = None
                 except (exception.CinderConnectionFailed,
                         exception.InvalidVolume):
                     raise
@@ -1930,6 +1937,11 @@ class API(base.Base):
             # instance is now in a cell and the delete needs to proceed
             # normally.
             return False
+
+        # We need to detach from any volumes so they aren't orphaned.
+        self._local_cleanup_bdm_volumes(
+            build_req.block_device_mappings, instance, context)
+
         return True
 
     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
@@ -1938,12 +1950,12 @@ class API(base.Base):
             return
 
         cell = None
-        # If there is an instance.host (or the instance is shelved-offloaded),
-        # the instance has been scheduled and sent to a cell/compute which
-        # means it was pulled from the cell db.
+        # If there is an instance.host (or the instance is shelved-offloaded or
+        # in error state), the instance has been scheduled and sent to a
+        # cell/compute which means it was pulled from the cell db.
         # Normal delete should be attempted.
-        if not (instance.host or
-                instance.vm_state == vm_states.SHELVED_OFFLOADED):
+        may_have_ports_or_volumes = self._may_have_ports_or_volumes(instance)
+        if not instance.host and not may_have_ports_or_volumes:
             try:
                 if self._delete_while_booting(context, instance):
                     return
@@ -2039,9 +2051,7 @@ class API(base.Base):
                 # which will cause a cast to the child cell.
                 cb(context, instance, bdms)
                 return
-            shelved_offloaded = (instance.vm_state
-                                 == vm_states.SHELVED_OFFLOADED)
-            if not instance.host and not shelved_offloaded:
+            if not instance.host and not may_have_ports_or_volumes:
                 try:
                     compute_utils.notify_about_instance_usage(
                             self.notifier, context, instance,
@@ -2056,7 +2066,12 @@ class API(base.Base):
                              {'state': instance.vm_state},
                               instance=instance)
                     return
-                except exception.ObjectActionError:
+                except exception.ObjectActionError as ex:
+                    # The instance's host likely changed under us as
+                    # this instance could be building and has since been
+                    # scheduled. Continue with attempts to delete it.
+                    LOG.debug('Refreshing instance because: %s', ex,
+                              instance=instance)
                     instance.refresh()
 
             if instance.vm_state == vm_states.RESIZED:
@@ -2064,7 +2079,8 @@ class API(base.Base):
 
             is_local_delete = True
             try:
-                if not shelved_offloaded:
+                # instance.host must be set in order to look up the service.
+                if instance.host is not None:
                     service = objects.Service.get_by_compute_host(
                         context.elevated(), instance.host)
                     is_local_delete = not self.servicegroup_api.service_is_up(
@@ -2081,7 +2097,9 @@ class API(base.Base):
 
                     cb(context, instance, bdms)
             except exception.ComputeHostNotFound:
-                pass
+                LOG.debug('Compute host %s not found during service up check, '
+                          'going to local delete instance', instance.host,
+                          instance=instance)
 
             if is_local_delete:
                 # If instance is in shelved_offloaded state or compute node
@@ -2108,6 +2126,16 @@ class API(base.Base):
             # NOTE(comstud): Race condition. Instance already gone.
             pass
 
+    def _may_have_ports_or_volumes(self, instance):
+        # NOTE(melwitt): When an instance build fails in the compute manager,
+        # the instance host and node are set to None and the vm_state is set
+        # to ERROR. In the case, the instance with host = None has actually
+        # been scheduled and may have ports and/or volumes allocated on the
+        # compute node.
+        if instance.vm_state in (vm_states.SHELVED_OFFLOADED, vm_states.ERROR):
+            return True
+        return False
+
     def _confirm_resize_on_deleting(self, context, instance):
         # If in the middle of a resize, use confirm_resize to
         # ensure the original instance is cleaned up too
@@ -2163,6 +2191,14 @@ class API(base.Base):
                           'the instance host %(instance_host)s.',
                           {'connector_host': connector.get('host'),
                            'instance_host': instance.host}, instance=instance)
+                if (instance.host is None and
+                        self._may_have_ports_or_volumes(instance)):
+                    LOG.debug('Allowing use of stashed volume connector with '
+                              'instance host None because instance with '
+                              'vm_state %(vm_state)s has been scheduled in '
+                              'the past.', {'vm_state': instance.vm_state},
+                              instance=instance)
+                    return connector
 
     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
         """The method deletes the bdm records and, if a bdm is a volume, call
@@ -2197,7 +2233,12 @@ class API(base.Base):
                 except Exception as exc:
                     LOG.warning("Ignoring volume cleanup failure due to %s",
                                 exc, instance=instance)
-            bdm.destroy()
+            # If we're cleaning up volumes from an instance that wasn't yet
+            # created in a cell, i.e. the user deleted the server while
+            # the BuildRequest still existed, then the BDM doesn't actually
+            # exist in the DB to destroy it.
+            if 'id' in bdm:
+                bdm.destroy()
 
     def _local_delete(self, context, instance, bdms, delete_type, cb):
         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
diff --git a/nova/tests/unit/compute/test_compute_api.py b/nova/tests/unit/compute/test_compute_api.py
index b82b410..2848c9e 100644
--- a/nova/tests/unit/compute/test_compute_api.py
+++ b/nova/tests/unit/compute/test_compute_api.py
@@ -967,7 +967,7 @@ class _ComputeAPIUnitTestMixIn(object):
         if self.cell_type != 'api':
             if inst.vm_state == vm_states.RESIZED:
                 self._test_delete_resized_part(inst)
-            if inst.vm_state != vm_states.SHELVED_OFFLOADED:
+            if inst.host is not None:
                 self.context.elevated().AndReturn(self.context)
                 objects.Service.get_by_compute_host(self.context,
                         inst.host).AndReturn(objects.Service())
@@ -975,9 +975,7 @@ class _ComputeAPIUnitTestMixIn(object):
                         mox.IsA(objects.Service)).AndReturn(
                                 inst.host != 'down-host')
 
-            if (inst.host == 'down-host' or
-                    inst.vm_state == vm_states.SHELVED_OFFLOADED):
-
+            if inst.host == 'down-host' or inst.host is None:
                 self._test_downed_host_part(inst, updates, delete_time,
                                             delete_type)
                 cast = False
-- 
2.7.4

