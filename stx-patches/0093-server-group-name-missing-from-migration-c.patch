From b923d4cdef74a0b1494ee1b8c0f00aa69d9beb16 Mon Sep 17 00:00:00 2001
From: Gerry Kopec <Gerry.Kopec@windriver.com>
Date: Sat, 3 Feb 2018 17:59:13 -0500
Subject: [PATCH 093/143] server group name missing from migration
 created request spec

When an instance is created in Kilo, no request spec is created as this
wasn't implemented in Kilo.  When we upgrade this instance from Kilo to Mitaka
and then to Newton, a request spec will be generated by one of the online
data migrations run as we set up the Newton controller - see
nova.objects.request_spec.migrate_instances_add_request_spec().  This was
an upstream add in Newton.

If the instance is within a server group, it will create a server group
in the request_spec.  However, this server group entry will not contain
the name of the server group.  With the changes for commit
8bf1d02d which added ordered scheduling during live migration for
instances within a server group, we access the server group name in the
request spec.  This results in an exception if we attempt to live migrate
one of these instances with an online migration created request specs.
Fix is to pass the server group name through the instance group setup
utilities.

If the instance is launched in Mitaka or Newton, the instance group in the
request spec is set up with the name at launch time so this scenario
won't be triggered.

During the next rebase, this fix should be merged with:
d9f6f1ca primary: server-groups best-effort and group size
---
 nova/objects/request_spec.py                      | 2 ++
 nova/scheduler/utils.py                           | 7 ++++---
 nova/tests/unit/scheduler/test_scheduler_utils.py | 5 +++--
 3 files changed, 9 insertions(+), 5 deletions(-)

diff --git a/nova/objects/request_spec.py b/nova/objects/request_spec.py
index ee65285..a3ca126 100644
--- a/nova/objects/request_spec.py
+++ b/nova/objects/request_spec.py
@@ -225,10 +225,12 @@ class RequestSpec(base.NovaObject):
             policies = list(filter_properties.get('group_policies'))
             hosts = list(filter_properties.get('group_hosts'))
             members = list(filter_properties.get('group_members'))
+            name = filter_properties.get('group_name')
             md = filter_properties.get('group_metadetails')
             self.instance_group = objects.InstanceGroup(policies=policies,
                                                         hosts=hosts,
                                                         members=members,
+                                                        name=name,
                                                         metadetails=md)
             # hosts has to be not part of the updates for saving the object
             self.instance_group.obj_reset_changes(['hosts'])
diff --git a/nova/scheduler/utils.py b/nova/scheduler/utils.py
index 9429325..157f318 100644
--- a/nova/scheduler/utils.py
+++ b/nova/scheduler/utils.py
@@ -52,7 +52,7 @@ CONF = nova.conf.CONF
 # WRS:extension - new options: metadetails
 GroupDetails = collections.namedtuple('GroupDetails', ['hosts', 'policies',
                                                        'members',
-                                                       'metadetails'])
+                                                       'metadetails', 'name'])
 
 
 # WRS:extension - extended rejection error with reasons
@@ -700,7 +700,7 @@ def _get_group_details(context, instance_uuid, request_spec,
 
         return GroupDetails(hosts=user_hosts | group_hosts,
                             policies=group.policies, members=group.members,
-                            metadetails=md)
+                            metadetails=md, name=group.name)
 
 
 def setup_instance_group(context, request_spec):
@@ -723,8 +723,9 @@ def setup_instance_group(context, request_spec):
         request_spec.instance_group.policies = group_info.policies
         request_spec.instance_group.members = group_info.members
 
-        # extension -- metadetails, group_exceed
+        # extension -- metadetails, name
         request_spec.instance_group.metadetails = group_info.metadetails
+        request_spec.instance_group.name = group_info.name
 
 
 def retry_on_timeout(retries=1):
diff --git a/nova/tests/unit/scheduler/test_scheduler_utils.py b/nova/tests/unit/scheduler/test_scheduler_utils.py
index 39f7804..43ed372 100644
--- a/nova/tests/unit/scheduler/test_scheduler_utils.py
+++ b/nova/tests/unit/scheduler/test_scheduler_utils.py
@@ -316,7 +316,7 @@ class SchedulerUtilsTestCase(test.NoDBTestCase):
                 (set(['hostA', 'hostB']), [policy], group.members,
                 {"wrs-sg:best_effort": "true",
                  "wrs-sg:group_size": "2",
-                 "wrs-sg:group_exceed": "0"}),
+                 "wrs-sg:group_exceed": "0"}, 'pele'),
                 group_info)
 
     def test_get_group_details(self):
@@ -371,7 +371,7 @@ class SchedulerUtilsTestCase(test.NoDBTestCase):
                        "wrs-sg:group_exceed": "0"}
         mock_ggd.return_value = scheduler_utils.GroupDetails(
             hosts=set(['hostA', 'hostB']), policies=['policy'],
-            members=['instance1'], metadetails=metadetails)
+            members=['instance1'], metadetails=metadetails, name='pele')
         spec = objects.RequestSpec(instance_uuid=uuids.instance)
         spec.instance_group = objects.InstanceGroup(hosts=['hostC'])
         spec.num_instances = 1
@@ -386,6 +386,7 @@ class SchedulerUtilsTestCase(test.NoDBTestCase):
         self.assertEqual(['policy'], spec.instance_group.policies)
         self.assertEqual(['instance1'], spec.instance_group.members)
         self.assertEqual(metadetails, spec.instance_group.metadetails)
+        self.assertEqual('pele', spec.instance_group.name)
 
     # WRS:extension
     @mock.patch.object(scheduler_utils, '_get_group_details')
-- 
2.7.4

